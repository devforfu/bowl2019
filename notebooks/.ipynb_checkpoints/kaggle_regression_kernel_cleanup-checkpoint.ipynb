{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupytools.syspath\n",
    "jupytools.syspath.add('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict, Counter, OrderedDict\n",
    "from itertools import chain\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "from basedir import TRAIN, TEST\n",
    "from dataset import load, load_sample, Subset\n",
    "from metric import qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11341042, 11) (17690, 7) (386, 3) "
     ]
    }
   ],
   "source": [
    "trn_data, trn_targ, trn_spec = load(Subset.Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1156414, 11) "
     ]
    }
   ],
   "source": [
    "[tst_data] = load(Subset.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(dataframes, key):\n",
    "    return list(set(chain(*[df[key].unique().tolist() for df in dataframes])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(dataframe, key):\n",
    "    return dataframe[key].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_tuple(name, **params):\n",
    "    from collections import namedtuple\n",
    "    return namedtuple(name, params.keys())(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_list(seq): return list(sorted(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(trn, tst, target):\n",
    "    trn, tst, target = [x.copy() for x in (trn, tst, target)]\n",
    "    trn['title_event_code'] = trn['title'].str.cat(trn['event_code'].astype(str), '_')\n",
    "    tst['title_event_code'] = tst['title'].str.cat(tst['event_code'].astype(str), '_')\n",
    "    data = [trn, tst]\n",
    "    \n",
    "    title_event_codes = unique(data, key='title_event_code')\n",
    "    titles = unique(data, key='title')\n",
    "    event_codes = unique(data, key='event_code')\n",
    "    event_ids = unique(data, key='event_id')\n",
    "    worlds = unique(data, key='world')\n",
    "    \n",
    "    title_enc = {x: i for i, x in enumerate(titles)}\n",
    "    title_dec = dict(enumerate(titles))\n",
    "    world_enc = {x: i for i, x in enumerate(worlds)}\n",
    "    \n",
    "    trn_asm = trn.query('type == \"Assessment\"')\n",
    "    tst_asm = tst.query('type == \"Assessment\"')\n",
    "    assessment_titles = set(count(trn_asm, 'title')).union(count(tst_asm, 'title'))\n",
    "\n",
    "    trn['title'] = trn['title'].map(title_enc)\n",
    "    tst['title'] = tst['title'].map(title_enc)\n",
    "    trn['world'] = trn['world'].map(world_enc)\n",
    "    tst['world'] = tst['world'].map(world_enc)\n",
    "    target['title'] = target['title'].map(title_enc)\n",
    "    \n",
    "    win_code_enc = dict(zip(title_enc.values(), [4100 for _ in title_enc]))\n",
    "    win_code_enc[title_enc['Bird Measurer (Assessment)']] = 4110\n",
    "    \n",
    "    trn['timestamp'] = pd.to_datetime(trn['timestamp'])\n",
    "    tst['timestamp'] = pd.to_datetime(tst['timestamp'])\n",
    "    \n",
    "    data = named_tuple('Data', x_train=trn, y_train=target, x_test=tst)\n",
    "    \n",
    "    meta = named_tuple('Meta', title_event_codes=title_event_codes,\n",
    "                       titles=as_list(titles), event_codes=as_list(event_codes), \n",
    "                       event_ids=as_list(event_ids), worlds=as_list(worlds),\n",
    "                       assessment_titles=as_list(assessment_titles),\n",
    "                       win_code_enc=win_code_enc, title_enc=title_enc,\n",
    "                       title_dec=title_dec, world_enc=world_enc)\n",
    "    \n",
    "    return data, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, meta = clean_up(trn_data, tst_data, trn_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bird Measurer (Assessment)',\n",
       " 'Cart Balancer (Assessment)',\n",
       " 'Cauldron Filler (Assessment)',\n",
       " 'Chest Sorter (Assessment)',\n",
       " 'Mushroom Sorter (Assessment)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.assessment_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savediv(a, b, fallback=0): \n",
    "    return a/b if b != 0 else fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_accuracy_group(accuracy):\n",
    "    return (0 if accuracy == 0 else \n",
    "            3 if accuracy == 1 else \n",
    "            2 if accuracy == 0.5 else\n",
    "            1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dict(keys, init_value=0):\n",
    "    return {k: init_value for k in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(user, meta, test=False):\n",
    "    last_activity = 0\n",
    "    last_session_time_sec = 0\n",
    "    acc_accuracy_group = 0\n",
    "    acc_accuracy = 0\n",
    "    acc_attempts_correct = 0\n",
    "    acc_attempts_incorrect = 0\n",
    "    acc_actions = 0\n",
    "    session_no = 0\n",
    "\n",
    "    last_accuracy_title = init_dict([f'acc_{t}' for t in meta.assessment_titles], -1)\n",
    "    accuracy_groups = init_dict([0, 1, 2, 3])\n",
    "    user_activities_cnt = init_dict(['Clip', 'Activity', 'Assessment', 'Game'])\n",
    "    event_code_cnt = init_dict(meta.event_codes)\n",
    "    event_id_cnt = init_dict(meta.event_ids)\n",
    "    title_cnt = init_dict(meta.titles)\n",
    "    title_event_code_cnt = init_dict(meta.title_event_codes)\n",
    "    \n",
    "    time_first_activity = float(user['timestamp'].values[0])\n",
    "    assessments = []\n",
    "    durations = []\n",
    "    \n",
    "    for _, session in user.groupby('game_session', sort=False):\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_installation_id = session['installation_id'].iloc[0]\n",
    "        session_title_text = meta.title_dec[session_title]\n",
    "        \n",
    "        if session_type == 'Assessment' and (test or len(session) > 1):\n",
    "            attempts = session.query(f'event_code == {meta.win_code_enc[session_title]}')\n",
    "            t_attempts = attempts['event_data'].str.contains('true').sum()\n",
    "            f_attempts = attempts['event_data'].str.contains('false').sum()\n",
    "\n",
    "            features = OrderedDict()\n",
    "            features.update(user_activities_cnt.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_cnt.copy())\n",
    "            features.update(event_id_cnt.copy())\n",
    "            features.update(title_cnt.copy())\n",
    "            features.update(title_event_code_cnt.copy())\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            \n",
    "            features['installation_id'] = session_installation_id\n",
    "            features['session_title'] = session_title\n",
    "            features['acc_attempts_correct'] = acc_attempts_correct\n",
    "            features['acc_attempts_incorrect'] = acc_attempts_incorrect\n",
    "            acc_attempts_correct += t_attempts\n",
    "            acc_attempts_incorrect += f_attempts\n",
    "            \n",
    "            features['duration_mean'] = np.mean(durations) if durations else 0\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2]).seconds)\n",
    "            \n",
    "            features['acc_accuracy'] = savediv(acc_accuracy, session_no)\n",
    "            accuracy = savediv(t_attempts, t_attempts + f_attempts)\n",
    "            acc_accuracy += accuracy\n",
    "            last_accuracy_title[f'acc_{session_title_text}'] = accuracy\n",
    "            features['accuracy_group'] = to_accuracy_group(accuracy)\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            features['acc_accuracy_group'] = savediv(acc_accuracy_group, session_no)\n",
    "            acc_accuracy_group += features['accuracy_group']\n",
    "            features['acc_actions'] = acc_actions\n",
    "            \n",
    "            if test or (t_attempts + f_attempts) > 0:\n",
    "                assessments.append(features)\n",
    "            \n",
    "            session_no += 1\n",
    "            \n",
    "        def update_counters(counter, column):\n",
    "            num_of_sessions = Counter(session[column])\n",
    "            for k1 in num_of_sessions.keys():\n",
    "                k2 = meta.title_dec[k1] if column == 'title' else k1\n",
    "                counter[k2] += num_of_sessions[k1]\n",
    "        \n",
    "        update_counters(event_code_cnt, 'event_code')\n",
    "        update_counters(event_id_cnt, 'event_id')\n",
    "        update_counters(title_cnt, 'title')\n",
    "        update_counters(title_event_code_cnt, 'title_event_code')\n",
    "        \n",
    "        acc_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_cnt[session_type] += 1\n",
    "            last_activity = session_type\n",
    "    \n",
    "    return [assessments[-1]] if test else assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_tqdm(data, wrap=True, **params):\n",
    "    return tqdm(data, **params) if wrap else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_groups(data, key, meta, \n",
    "                                 test=False, pbar=True,\n",
    "                                 num_workers=cpu_count()):\n",
    "    \n",
    "    grouped = data.groupby(key, sort=False)\n",
    "    groups = (g for _, g in grouped)\n",
    "    n_total = grouped.ngroups\n",
    "    groups = wrap_tqdm(\n",
    "        groups, wrap=pbar, total=n_total,\n",
    "        desc='Test' if test else 'Train')\n",
    "    with Parallel(num_workers) as p:\n",
    "        results = p(delayed(extract_features)(g, meta, test) for g in groups)\n",
    "    return pd.DataFrame(list(chain(*results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(trn, tst, meta, pbar=True, num_workers=cpu_count()):\n",
    "    trn = extract_features_from_groups(\n",
    "        trn, meta=meta, key='installation_id', num_workers=num_workers)\n",
    "    tst = extract_features_from_groups(\n",
    "        tst, meta=meta, key='installation_id', num_workers=num_workers,\n",
    "        test=True)\n",
    "    return trn, tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfdf8806c0a43a087fa15eb5027504e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train', max=17000, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d24078cc88403bb97de861791c4f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Test', max=1000, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_trn, X_tst = create_train_test(data.x_train, data.x_test, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_post_processing_features(trn, tst, meta):\n",
    "    for df in (trn, tst):\n",
    "        grouped = df.groupby('installation_id')\n",
    "        df['installation_session_count'] = grouped['Clip'].transform('count')\n",
    "        df['installation_duration_mean'] = grouped['duration_mean'].transform('mean')\n",
    "        df['installation_title_nunique'] = grouped['session_title'].transform('nunique')\n",
    "        df['event_code_count_sum'] = df[list(meta.event_codes)].sum(axis=1)\n",
    "        df['installation_event_code_count_mean'] = (\n",
    "            df.groupby('installation_id')['event_code_count_sum'].transform('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_with_post_processing_features(X_trn, X_tst, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_features(trn, tst):\n",
    "    nonzero_rows = trn.sum(axis=1) != 0\n",
    "    nonzero_cols = trn.sum(axis=0) != 0\n",
    "    features = trn.loc[nonzero_rows, nonzero_cols].columns.tolist()\n",
    "    features = (\n",
    "        [f for f in features if f not in ('accuracy_group', 'installation_id')] +\n",
    "        [f'acc_{t}' for t in meta.assessment_titles])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_features = get_relevant_features(X_trn, X_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(available_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.displayplay import HTML\n",
    "table = '<thead><th>Feature Name</th></thead>'\n",
    "for featureavailable_featuresilable_featuresavailable_features:\n",
    "    table += f'<tr><td>{feature}</td></tr>'\n",
    "HTML(f'<table>{table}</table>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import qwk\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "# [-inf, 1.0272319526554337, 1.7321346312545782, 2.238358126272799, inf]\n",
    "\n",
    "# def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "#     y_pred = y_pred.copy()\n",
    "#     y_pred[y_pred <= 1.12232214] = 0\n",
    "#     y_pred[np.where(np.logical_and(y_pred > 1.12232214, y_pred <= 1.73925866))] = 1\n",
    "#     y_pred[np.where(np.logical_and(y_pred > 1.73925866, y_pred <= 2.22506454))] = 2\n",
    "#     y_pred[y_pred > 2.22506454] = 3\n",
    "#     return 'cappa', qwk(y_true, y_pred), True\n",
    "\n",
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    y_pred = y_pred.copy()\n",
    "    y_pred[y_pred <= 1.0272319526554337] = 0\n",
    "    y_pred[np.where(np.logical_and(y_pred > 1.0272319526554337, y_pred <= 1.7321346312545782))] = 1\n",
    "    y_pred[np.where(np.logical_and(y_pred > 1.7321346312545782, y_pred <= 2.238358126272799))] = 2\n",
    "    y_pred[y_pred > 2.238358126272799] = 3\n",
    "    return 'cappa', qwk(y_true, y_pred), True\n",
    "\n",
    "\n",
    "k = 5\n",
    "folds = GroupKFold(n_splits=k)\n",
    "groups = X_trn['installation_id']\n",
    "X = X_trn[available_features].copy()\n",
    "y = X_trn['accuracy_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_estimators=2000,\n",
    "              boosting_type='gbdt',\n",
    "              objective='regression',\n",
    "              metric='rmse',\n",
    "              subsample=0.75,\n",
    "              subsample_freq=1,\n",
    "              learning_rate=0.04,\n",
    "              feature_fraction=0.9,\n",
    "              max_depth=15,\n",
    "              lambda_l1=1,\n",
    "              lambda_l2=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k-fold 1 of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrn's rmse: 0.908413\ttrn's cappa: 0.676305\tval's rmse: 0.978849\tval's cappa: 0.610315\n",
      "[200]\ttrn's rmse: 0.842117\ttrn's cappa: 0.736391\tval's rmse: 0.971586\tval's cappa: 0.61322\n",
      "[300]\ttrn's rmse: 0.792227\ttrn's cappa: 0.773537\tval's rmse: 0.971217\tval's cappa: 0.617272\n",
      "[400]\ttrn's rmse: 0.752169\ttrn's cappa: 0.800877\tval's rmse: 0.972938\tval's cappa: 0.61434\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttrn's rmse: 0.810635\ttrn's cappa: 0.759768\tval's rmse: 0.971524\tval's cappa: 0.619668\n",
      "Running k-fold 2 of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrn's rmse: 0.906489\ttrn's cappa: 0.680689\tval's rmse: 0.9855\tval's cappa: 0.608238\n",
      "[200]\ttrn's rmse: 0.841001\ttrn's cappa: 0.735247\tval's rmse: 0.979902\tval's cappa: 0.61491\n",
      "[300]\ttrn's rmse: 0.792855\ttrn's cappa: 0.772451\tval's rmse: 0.979159\tval's cappa: 0.614939\n",
      "[400]\ttrn's rmse: 0.753017\ttrn's cappa: 0.801028\tval's rmse: 0.979735\tval's cappa: 0.616414\n",
      "Early stopping, best iteration is:\n",
      "[223]\ttrn's rmse: 0.828496\ttrn's cappa: 0.74308\tval's rmse: 0.978559\tval's cappa: 0.618104\n",
      "Running k-fold 3 of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrn's rmse: 0.908738\ttrn's cappa: 0.680009\tval's rmse: 0.978143\tval's cappa: 0.583708\n",
      "[200]\ttrn's rmse: 0.842037\ttrn's cappa: 0.736319\tval's rmse: 0.973784\tval's cappa: 0.594276\n",
      "[300]\ttrn's rmse: 0.793363\ttrn's cappa: 0.774545\tval's rmse: 0.975101\tval's cappa: 0.594888\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttrn's rmse: 0.844783\ttrn's cappa: 0.733338\tval's rmse: 0.973541\tval's cappa: 0.595591\n",
      "Running k-fold 4 of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrn's rmse: 0.901468\ttrn's cappa: 0.68234\tval's rmse: 1.00196\tval's cappa: 0.586665\n",
      "[200]\ttrn's rmse: 0.835795\ttrn's cappa: 0.737575\tval's rmse: 0.997937\tval's cappa: 0.594791\n",
      "[300]\ttrn's rmse: 0.78728\ttrn's cappa: 0.777291\tval's rmse: 0.998297\tval's cappa: 0.594568\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttrn's rmse: 0.848076\ttrn's cappa: 0.727694\tval's rmse: 0.99693\tval's cappa: 0.59422\n",
      "Running k-fold 5 of 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttrn's rmse: 0.898629\ttrn's cappa: 0.685093\tval's rmse: 1.01002\tval's cappa: 0.579387\n",
      "[200]\ttrn's rmse: 0.832056\ttrn's cappa: 0.741143\tval's rmse: 1.00666\tval's cappa: 0.58372\n",
      "[300]\ttrn's rmse: 0.783504\ttrn's cappa: 0.777503\tval's rmse: 1.00925\tval's cappa: 0.580163\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttrn's rmse: 0.855268\ttrn's cappa: 0.722094\tval's rmse: 1.00717\tval's cappa: 0.588968\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "models = []\n",
    "oof = np.zeros(X.shape[0], dtype=np.float32)\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups), 1):\n",
    "    print(f'Running k-fold {i} of {k}')\n",
    "    x_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "    x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(x_trn, y_trn,\n",
    "              eval_set=[(x_trn, y_trn), (x_val, y_val)], \n",
    "              eval_names=['trn', 'val'],\n",
    "              eval_metric=eval_qwk_lgb_regr, early_stopping_rounds=200,\n",
    "              verbose=100, categorical_feature='auto')\n",
    "    oof[val_idx] = model.predict(x_val)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cappa', 0.6036508470866481, True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_qwk_lgb_regr(y, oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ck/data/bowl2019/external/models_lightgbm_002.joblib']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(models, '/home/ck/data/bowl2019/external/models_lightgbm_002.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ck/data/bowl2019/external/available_features.joblib']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(available_features, '/home/ck/data/bowl2019/external/available_features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ck/data/bowl2019/external/meta_dict.joblib']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(meta._asdict(), '/home/ck/data/bowl2019/external/meta_dict.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((X_tst.shape[0], len(models)), dtype=np.float32)\n",
    "for i, model in enumerate(models):\n",
    "    preds[:, i] = model.predict(X_tst[available_features])\n",
    "avg_preds = np.mean(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def optimize_rounding_bounds(X, y):\n",
    "    def _loss(coef):\n",
    "        buckets = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n",
    "        return -qwk(y, buckets)\n",
    "    \n",
    "    init_coef = [0.5, 1.5, 2.5]\n",
    "    opt_coef = scipy.optimize.minimize(_loss, init_coef, method='nelder-mead')\n",
    "    optimized = opt_coef['x']\n",
    "    return [-np.inf] + optimized.tolist() + [np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(oof) == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = optimize_rounding_bounds(oof, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-inf, 1.0272319526554337, 1.7321346312545782, 2.238358126272799, inf]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_regressor_predictions(preds, coefs):\n",
    "    x = preds.copy()\n",
    "    for i, (lo, hi) in enumerate(zip(coefs[:-1], coefs[1:])):\n",
    "        x[(x > lo) & (x <= hi)] = i\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = round_regressor_predictions(avg_preds, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SAMPLE\n",
    "sample = pd.read_csv(SAMPLE)\n",
    "sample['accuracy_group'] = y_hat.astype(int)\n",
    "sample.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(value, fallback=0):\n",
    "    return value if value is not None else fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatetimeFeatures:\n",
    "    def __init__(self, prefix=None, field_name='timestamp', \n",
    "                 attributes=('Year', 'Month', 'Week', 'Day', 'Dayofweek'),\n",
    "                 drop=True, date=True, time=True):\n",
    "        self.prefix = default(prefix, re.sub('[Dd]ate$', '', field_name))\n",
    "        self.field_name = field_name\n",
    "        self.attributes = attributes\n",
    "        self.drop = drop\n",
    "        self.date = date\n",
    "        self.time = time\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        field = X[self.field_name]\n",
    "        attrs = list(self.attributes)\n",
    "        if date:\n",
    "            attrs.append('Date')\n",
    "        if time:\n",
    "            attrs.extend(['Hour', 'Minute'])\n",
    "        for attr in attrs:\n",
    "            X[f'{prefix}{attr}'] = getattr(field.dt, attr.lower())\n",
    "        if drop:\n",
    "            X = X.drop(field_name, axis=1)\n",
    "        return X\n",
    "    def fit_transform(self, X, y=None):\n",
    "        data = copy.deepcopy(X)\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionFeatures:\n",
    "    def __init__(self, n_interactions=20):\n",
    "        self.n_interactions = n_interactions\n",
    "        self.features = None\n",
    "        self.interact_1 = None\n",
    "        self.interact_2 = None\n",
    "    def fit(self, X, y=None):\n",
    "        substrings = ('sum', 'mean', 'max', 'std', 'attempt')\n",
    "        self.features = [col for col in X.columns if any(x in col for x in substrings)]\n",
    "        self.interact_1 = np.random.choice(self.features, self.n_interactions)\n",
    "        self.interact_2 = np.random.choice(self.features, self.n_interactions)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        for col1 in self.interact_1:\n",
    "            for col2 in self.interact_2:\n",
    "                data[f'interact_{col1}_{col2}'] = data[col1] * data[col2]\n",
    "        return data\n",
    "    def fit_transform(self, X, y=None, **params):\n",
    "        data = copy.deepcopy(X)\n",
    "        return self.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
