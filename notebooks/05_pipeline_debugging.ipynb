{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import jupytools.syspath\n",
    "def ignore(*args, **kwargs): pass\n",
    "warnings.warn = ignore\n",
    "jupytools.syspath.add('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "from os.path import join\n",
    "\n",
    "import feather\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import bundle\n",
    "import utils as U\n",
    "from dataset import load, load_sample, Subset, to_accuracy_group\n",
    "from metric import qwk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending The Dataset\n",
    "\n",
    "The original datasets are processed as the whole to extend them with additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_combinations(data, pairs):\n",
    "    for c1, c2 in pairs:\n",
    "        assert c1 in data.columns, f'Column not found: {c1}'\n",
    "        assert c2 in data.columns, f'Column not found: {c2}'\n",
    "        data[f'{c1}_{c2}'] = data[c1].astype(str).str.cat(data[c2].astype(str), '_')\n",
    "    return data\n",
    "\n",
    "def add_datetime(data, column, prefix=None, with_time=True):\n",
    "    data[column] = pd.to_datetime(data[column])\n",
    "    prefix = U.default(prefix, re.sub('[Dd]ate$', '', column))\n",
    "    attrs = ('Year', 'Month', 'Week', 'Day', 'Dayofweek')\n",
    "    if with_time:\n",
    "        attrs += ('Hour', 'Minute')\n",
    "    for attr in attrs:\n",
    "        data[f'{prefix}_{attr}'] = getattr(data[column].dt, attr.lower())\n",
    "    return data\n",
    "\n",
    "def add_cyclical(data, prefix, features=('Year', 'Month', 'Week', 'Hour', 'Minute'),\n",
    "                 modulo=None):\n",
    "    modulo = modulo or {}\n",
    "    for feature in features:\n",
    "        column = f'{prefix}_{feature}'\n",
    "        m = modulo.get(feature, 23.0)\n",
    "        data[f'{column}_sin'] = np.sin(2*np.pi*data[column] / m)\n",
    "        data[f'{column}_cos'] = np.cos(2*np.pi*data[column] / m)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Meta Information\n",
    "\n",
    "The meta-information is computed using public train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meta_data(dataset, *datasets):\n",
    "    datasets = [dataset] + list(datasets)\n",
    "    uniq = OrderedDict()\n",
    "    uniq['title_event_code'] = U.unique(datasets, column='title_event_code')\n",
    "    uniq['title'] = U.unique(datasets, column='title')\n",
    "    uniq['event_code'] = U.unique(datasets, column='event_code')\n",
    "    uniq['event_id'] = U.unique(datasets, column='event_id')\n",
    "    uniq['world'] = U.unique(datasets, column='world')\n",
    "    uniq['type'] = U.unique(datasets, column='type')\n",
    "    asm_datasets = [ds.query('type == \"Assessment\"') for ds in datasets]\n",
    "    uniq['assessment_titles'] = U.unique(asm_datasets, column='title')\n",
    "    win_codes = {t: 4100 for t in uniq['title']}\n",
    "    win_codes['Bird Measurer (Assessment)'] = 4110\n",
    "    meta = {'win_codes': win_codes, **uniq}\n",
    "    return U.named_tuple('Meta', **meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Features\n",
    "\n",
    "Converts the raw dataset into user-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtractor:\n",
    "    def __init__(self, steps):\n",
    "        self.steps = steps\n",
    "        \n",
    "    def init_steps(self, meta):\n",
    "        for step in self.steps:\n",
    "            if hasattr(step, 'init'):\n",
    "                step.init(meta)\n",
    "                \n",
    "    def __call__(self, user, meta, test=False):\n",
    "        rows = []\n",
    "        self.init_steps(meta)\n",
    "        for _, session in user.groupby('game_session', sort=False):\n",
    "            info = session_info(session, meta, test)\n",
    "            features = OrderedDict([\n",
    "                ('installation_id', info.installation_id),\n",
    "                ('game_session', info.game_session),\n",
    "                ('session_title', info.session_title)\n",
    "            ])\n",
    "            for step in self.steps:\n",
    "                extracted = step.extract(session, info, meta)\n",
    "                features.update(extracted)\n",
    "            if info.should_include:\n",
    "                rows.append(features)\n",
    "        return [rows[-1]] if test else rows\n",
    "    \n",
    "def session_info(session, meta, test):\n",
    "    \"\"\"Computes information about user's session.\"\"\"\n",
    "    assert not session.empty, 'Session cannot be empty!'\n",
    "    session_type = session['type'].iloc[0]\n",
    "    assessment = session_type == 'Assessment'\n",
    "    outcomes = attempt_outcomes(session, meta) if assessment else None\n",
    "    should_include = (\n",
    "        (assessment and test) or\n",
    "        (assessment and (len(session) > 1) and outcomes.total > 0))\n",
    "    duration = session.timestamp.iloc[-1] - session.timestamp.iloc[0]\n",
    "    return U.named_tuple(\n",
    "        name='Info', \n",
    "        installation_id=session['installation_id'].iloc[0],\n",
    "        game_session=session['game_session'].iloc[0],\n",
    "        session_title=session['title'].iloc[0],\n",
    "        session_type=session_type,\n",
    "        is_assessment=assessment,\n",
    "        should_include=should_include,\n",
    "        outcomes=outcomes,\n",
    "        duration_seconds=duration.seconds)\n",
    "\n",
    "def attempt_outcomes(session, meta):\n",
    "    \"\"\"Computes how many successful and unsuccessful attempts contains the session.\"\"\"\n",
    "    event_code = meta.win_codes.get(session.title.iloc[0], 4100)\n",
    "    total_attempts = session.query(f'event_code == {event_code}')\n",
    "    pos = total_attempts.event_data.str.contains('true').sum()\n",
    "    neg = total_attempts.event_data.str.contains('false').sum()\n",
    "    summary = dict(pos=pos, neg=neg, total=(pos + neg))\n",
    "    return U.named_tuple('Trial', **summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFeatures:\n",
    "    def __init__(self, meta):\n",
    "        self.init(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingFeatures(BaseFeatures):\n",
    "    def init(self, meta):\n",
    "        self.cnt_title_event_code = U.init_dict(meta.title_event_code)\n",
    "        self.cnt_title = U.init_dict(meta.title)\n",
    "        self.cnt_event_code = U.init_dict(meta.event_code)\n",
    "        self.cnt_event_id = U.init_dict(meta.event_id)\n",
    "        self.cnt_activities = U.init_dict(meta.type)\n",
    "        self.last_activity = None\n",
    "        \n",
    "    def extract(self, session, info, meta):\n",
    "        features = OrderedDict()\n",
    "        if info.should_include:\n",
    "            counters = OrderedDict([\n",
    "                *self.cnt_title_event_code.items(),\n",
    "                *self.cnt_title.items(),\n",
    "                *self.cnt_event_code.items(),\n",
    "                *self.cnt_event_id.items(),\n",
    "                *self.cnt_activities.items()])\n",
    "            features.update([(f'cnt_{k}', v) for k, v in counters.items()])\n",
    "        self.update_counters(self.cnt_title_event_code, session, 'title_event_code')\n",
    "        self.update_counters(self.cnt_title, session, 'title')\n",
    "        self.update_counters(self.cnt_event_code, session, 'event_code')\n",
    "        self.update_counters(self.cnt_event_id, session, 'event_id')\n",
    "        if self.last_activity is None or self.last_activity != info.session_type:\n",
    "            self.cnt_activities[info.session_type] += 1\n",
    "            self.last_activity = info.session_type\n",
    "        return features\n",
    "        \n",
    "    def update_counters(self, cnt, sess, column):\n",
    "        uniq_counts = Counter(sess[column])\n",
    "        for k, v in uniq_counts.items():\n",
    "            if k in cnt:\n",
    "                cnt[k] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceFeatures(BaseFeatures):\n",
    "    def init(self, meta):\n",
    "        self.acc_accuracy = 0\n",
    "        self.acc_accuracy_group = 0\n",
    "        self.acc_correct_attempts = 0\n",
    "        self.acc_incorrect_attempts = 0\n",
    "        self.acc_actions = 0\n",
    "        self.durations = []\n",
    "        self.accuracy_groups = U.init_dict([0, 1, 2, 3])\n",
    "        self.last_accuracy_title = U.init_dict([f'acc_{t}' for t in meta.title], -1)\n",
    "        self.n_rows = 0\n",
    "    \n",
    "    def extract(self, session, info, meta):\n",
    "        features = OrderedDict()\n",
    "        \n",
    "        if info.should_include:\n",
    "            features['acc_attempts_pos'] = self.acc_correct_attempts\n",
    "            features['acc_attempts_neg'] = self.acc_incorrect_attempts\n",
    "            self.acc_correct_attempts += info.outcomes.pos\n",
    "            self.acc_incorrect_attempts += info.outcomes.neg\n",
    "            \n",
    "            features['acc_accuracy'] = U.savediv(self.acc_accuracy, self.n_rows)\n",
    "            accuracy = U.savediv(info.outcomes.pos, info.outcomes.total)\n",
    "            self.acc_accuracy += accuracy\n",
    "            \n",
    "            features.update(self.last_accuracy_title)\n",
    "            self.last_accuracy_title[f'acc_{info.session_title}'] = accuracy\n",
    "            \n",
    "            features['accuracy_group'] = to_accuracy_group(accuracy)\n",
    "            self.accuracy_groups[features['accuracy_group']] += 1\n",
    "            \n",
    "            features['acc_accuracy_group'] = U.savediv(self.acc_accuracy_group, self.n_rows)\n",
    "            self.acc_accuracy_group += features['accuracy_group']\n",
    "\n",
    "            features['acc_actions'] = self.acc_actions\n",
    "            \n",
    "            features['duration_mean'] = np.mean(self.durations) if self.durations else 0\n",
    "            self.durations.append(info.duration_seconds)\n",
    "            \n",
    "            self.n_rows += 1\n",
    "            \n",
    "        self.acc_actions += len(session)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset In Memory\n",
    "There are two possible algorithms to prepare the data before training:\n",
    "1. store every user subset on disk and process it from there,\n",
    "2. keep the whole dataset in memory and process without dumping on disk (local training).\n",
    "\n",
    "The first approach is more difficult, and requires extra steps between pipeline stages. Therefore, we go with the second one and hope that the data fits into memory on kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryAlgorithm:\n",
    "    def __init__(self, extractor, meta, pbar=True, num_workers=cpu_count()):\n",
    "        self.extractor = extractor\n",
    "        self.meta = meta\n",
    "        self.pbar = pbar\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "    def run(self, dataset, test=False):\n",
    "        mode = 'test' if test else 'train'\n",
    "        U.log(f'Running algorithm in {mode} mode.')\n",
    "        \n",
    "        def _extract(user):\n",
    "            return pd.DataFrame(self.extractor(user, self.meta, test))\n",
    "        \n",
    "        grouped = dataset.groupby('installation_id', sort=False)\n",
    "        users = (g for _, g in grouped)\n",
    "        if self.pbar:\n",
    "            users = tqdm(users, total= grouped.ngroups)\n",
    "        datasets = U.parallel(_extract, users, num_workers=self.num_workers)\n",
    "        dataset = pd.concat(datasets, axis=0)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(dataset, columns, encoders=None):\n",
    "    def make_encoder(mapping):\n",
    "        return lambda x: mapping.get(x, -1)\n",
    "    encoders = encoders or {}\n",
    "    for column in columns:\n",
    "        if column in encoders:\n",
    "            dataset[column] = dataset[column].map(make_encoder(encoders[column]))\n",
    "        else:\n",
    "            encoded, labels = pd.factorize(dataset[column])\n",
    "            encoder = OrderedDict([(x, i) for i, x in enumerate(labels)])\n",
    "            encoders[column] = encoder\n",
    "            dataset[column] = encoded\n",
    "    return dataset, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing Features\n",
    "Some features can be added only when user-wise features are created and represented as data frame. This features are created in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_wise_features(dataset, meta, pbar=True):\n",
    "    def transform(group_obj, key, agg): \n",
    "        return group_obj[key].transform(agg)\n",
    "    \n",
    "    events = [f'cnt_{code}' for code in meta.event_code]\n",
    "    grouped = dataset.groupby('installation_id')\n",
    "    dataset['user_session_cnt'] = transform(grouped, 'cnt_Clip', 'count')\n",
    "    dataset['user_duration_mean'] = transform(grouped, 'duration_mean', 'mean')\n",
    "    dataset['user_title_nunique'] = transform(grouped, 'session_title', 'nunique')\n",
    "    dataset['user_events_sum'] = dataset[events].sum(axis=1)\n",
    "    dataset['user_events_mean'] = transform(grouped, 'user_events_sum', 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Relevant Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_features(dataset):\n",
    "    U.log('Picking relevant features only.')\n",
    "    \n",
    "    def nonzero(x): return not np.allclose(x, 0)\n",
    "    columns = ['accuracy_group', 'installation_id', 'game_session']\n",
    "    dataset = dataset.drop(columns=columns)\n",
    "    nonzero_rows = dataset.sum(axis=1).map(nonzero)\n",
    "    nonzero_cols = dataset.sum(axis=0).map(nonzero)\n",
    "    features = dataset.loc[nonzero_rows, nonzero_cols].columns.tolist()\n",
    "    \n",
    "    before = len(dataset.columns)\n",
    "    after = len(features)\n",
    "    U.log(f'Number of features changed from {before} to {after}')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Into Prepared Pipeline\n",
    "Gathering all created functions into data preparing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = False\n",
    "if U.on_kaggle():\n",
    "    U.log('Loading test set only.')\n",
    "    [tst_data] = load(Subset.Test)\n",
    "else:\n",
    "    if sample:\n",
    "        U.log('Warning: loading train and test data sample.')\n",
    "        trn_data, _, _ = load_sample(Subset.Train, 500_000)\n",
    "        [tst_data] = load_sample(Subset.Test, 500_000)\n",
    "    else:\n",
    "        U.log('Loading train and test.')\n",
    "        trn_data, trn_spec, trn_targ = load(Subset.Train)\n",
    "        [tst_data] = load(Subset.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = U.combine(\n",
    "    partial(add_feature_combinations, pairs=[('title', 'event_code')]),\n",
    "    partial(add_datetime, column='timestamp', prefix='ts'),\n",
    "    partial(add_cyclical, prefix='ts'))\n",
    "\n",
    "if U.on_kaggle():\n",
    "    U.log('Transforming test data only.')\n",
    "    X_tst = transform(tst_data.copy())\n",
    "    U.log(X_tst.shape)\n",
    "else:\n",
    "    U.log('Transforming train and test data.')\n",
    "    X_tst = transform(tst_data.copy())\n",
    "    X_trn = transform(trn_data.copy())\n",
    "    U.log(X_trn.shape, X_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if U.on_kaggle():\n",
    "    U.log('Reading pre-computed meta from disk.')\n",
    "    meta = bundle.meta()\n",
    "else:\n",
    "    U.log('Computing meta using train and test datasets.')\n",
    "    meta = compute_meta_data(X_trn, X_tst)\n",
    "    U.log('Saving computed meta on disk.')\n",
    "    bundle.save_meta(meta, 'meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = FeaturesExtractor([\n",
    "    CountingFeatures(meta),\n",
    "    PerformanceFeatures(meta)\n",
    "])\n",
    "algo = InMemoryAlgorithm(extractor, meta)\n",
    "cat_cols = ['session_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if U.on_kaggle():\n",
    "    U.log('Preparing test dataset.')\n",
    "    X_tst = algo.run(X_tst, test=True)\n",
    "    encoders = bundle.encoders()\n",
    "    X_tst, _ = encode(X_tst, cat_cols, encoders=encoders)\n",
    "else:\n",
    "    U.log('Preparing train and test datasets.')\n",
    "    X_trn = algo.run(X_trn)\n",
    "    X_tst = algo.run(X_tst, test=True)\n",
    "    X_trn, encoders = encode(X_trn, cat_cols)\n",
    "    X_tst, _ = encode(X_tst, cat_cols, encoders=encoders)\n",
    "    bundle.save(encoders, 'encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if U.on_kaggle():\n",
    "    U.log('Running post-processing on test set only.')\n",
    "    add_user_wise_features(X_tst, meta)\n",
    "else:\n",
    "    U.log('Running post-processing on train and test sets.')\n",
    "    add_user_wise_features(X_trn, meta)\n",
    "    add_user_wise_features(X_tst, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if U.on_kaggle():\n",
    "    U.log('Loading relevant features list from disk.')\n",
    "    features = bundle.features()\n",
    "else:\n",
    "    U.log('Deriving relevant features from train dataset.')\n",
    "    features = get_relevant_features(X_trn)\n",
    "    bundle.save(features, 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBM:\n",
    "    def __init__(self, config):\n",
    "        self.model = lgb.LGBMRegressor(**config.get('model_params', {}))\n",
    "        self.config = config\n",
    "    def fit(self, train_data, valid_data, metric):\n",
    "        x_trn, y_trn = train_data\n",
    "        x_val, y_val = valid_data\n",
    "        params = self.config.get('fit_params', {}).copy()\n",
    "        params['eval_set'] = [(x_trn, y_trn), (x_val, y_val)]\n",
    "        params['eval_names'] = ['trn', 'val']\n",
    "        params['eval_metric'] = metric\n",
    "        params['X'] = x_trn\n",
    "        params['y'] = y_trn\n",
    "        self.model.fit(**params)\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "MODEL_CONFIG = dict(\n",
    "    lightgbm=dict(\n",
    "        model_params=dict(\n",
    "            n_estimators=2000,\n",
    "            max_depth=15,\n",
    "            metric='rmse',\n",
    "            objective='regression',\n",
    "            learning_rate=1e-2,\n",
    "        ),\n",
    "        fit_params=dict(\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=50,\n",
    "            categorical_feature='auto'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "def get_default_config(name):\n",
    "    return MODEL_CONFIG[name]\n",
    "\n",
    "def get_model_class(name):\n",
    "    if name == 'lightgbm': return LightGBM\n",
    "    raise ValueError(f'unknown model class: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(data, model='lightgbm', version='003', chunk_size=128):\n",
    "    U.log(f'Running inference on dataset of shape: {data.shape}')\n",
    "    indexes = np.arange(len(data))\n",
    "    U.log('Loading external models.')\n",
    "    models = bundle.models(model=model, version=version)\n",
    "    U.log('Loading external features.')\n",
    "    features = bundle.features()\n",
    "    preds = {i: [] for i, _ in enumerate(models)}\n",
    "    U.log('Running models on test data...')\n",
    "    for chunk in U.chunks(indexes, chunk_size):\n",
    "        x_test = data.iloc[chunk][features]\n",
    "        for i, model in enumerate(models):\n",
    "            pred = model.predict(x_test).tolist()\n",
    "            preds[i].extend(pred)\n",
    "    U.log('Averaging ensemble predictions.')\n",
    "    avg_preds = pd.DataFrame(preds).mean(axis=1).values\n",
    "    U.log('Rounding predictions using optimal bounds.')\n",
    "    y_hat = round_regressor_predictions(avg_preds, bundle.bounds())\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(predicted):\n",
    "    U.log('Converting predictions into submission file.')\n",
    "    if U.on_kaggle():\n",
    "        U.log('Running on Kaggle.')\n",
    "        sample = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    else:\n",
    "        U.log('Running locally.')\n",
    "        [sample] = load(Subset.Sample)\n",
    "    sample['accuracy_group'] = predicted.astype(int)\n",
    "    sample.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, features, reg_metric, algo='lightgbm', n_folds=5, config=None):\n",
    "    models = []\n",
    "    folds = GroupKFold(n_splits=n_folds)\n",
    "    groups = dataset['installation_id']\n",
    "    X = dataset[features].copy()\n",
    "    y = dataset['accuracy_group']\n",
    "    oof = np.zeros(X.shape[0], dtype=np.float32)\n",
    "    model_cls = get_model_class(algo)\n",
    "    \n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups), 1):\n",
    "        U.log(f'Running k-fold {i} of {n_folds}')\n",
    "        x_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "        x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        model = model_cls(config or get_default_config(algo))\n",
    "        model.fit(train_data=(x_trn, y_trn), \n",
    "                  valid_data=(x_val, y_val), \n",
    "                  metric=getattr(reg_metric, algo))\n",
    "        oof[val_idx] = model.predict(x_val)\n",
    "        models.append(model)\n",
    "        \n",
    "    return U.named_tuple('Result', models=models, oof=oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionCappa:\n",
    "    def __init__(self, bounds):\n",
    "        self.bounds = bounds\n",
    "    def lightgbm(self, y_true, y_pred):\n",
    "        y_rounded = round_regressor_predictions(y_pred, self.bounds)\n",
    "        return 'cappa', qwk(y_true, y_rounded), True\n",
    "    \n",
    "def round_regressor_predictions(preds, coefs):\n",
    "    x = preds.copy()\n",
    "    for i, (lo, hi) in enumerate(zip(coefs[:-1], coefs[1:])):\n",
    "        x[(x > lo) & (x <= hi)] = i\n",
    "    return x\n",
    "\n",
    "def optimize_rounding_bounds(X, y):\n",
    "    def _loss(coef):\n",
    "        buckets = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels=[0, 1, 2, 3])\n",
    "        return -qwk(y, buckets)\n",
    "    \n",
    "    init_coef = [0.5, 1.5, 2.5]\n",
    "    opt_coef = scipy.optimize.minimize(_loss, init_coef, method='nelder-mead')\n",
    "    optimized = opt_coef['x']\n",
    "    return [-np.inf] + optimized.tolist() + [np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'lightgbm'\n",
    "version = '004'\n",
    "\n",
    "if U.on_kaggle():\n",
    "    U.log('Inference on Kaggle.')\n",
    "    predicted = inference(X_tst, model=algo, version=version)\n",
    "    submit(predicted)\n",
    "    \n",
    "else:\n",
    "    U.log('Training with sub-optimal rounding.')\n",
    "    reg_metric = RegressionCappa([-np.inf, 1., 2., 3., +np.inf])\n",
    "    result = train(X_trn, features, reg_metric, algo=algo)\n",
    "    \n",
    "    U.log('Using predictions to find optimal rounding boundaries.')\n",
    "    opt_bounds = optimize_rounding_bounds(result.oof, X_trn['accuracy_group'])\n",
    "    U.log(f'Optimal values: {opt_bounds}')\n",
    "    \n",
    "    U.log('Using optimal boundaries to train a new ensemble of models.')\n",
    "    reg_metric = RegressionCappa(opt_bounds)\n",
    "    result = train(X_trn, features, reg_metric, algo=algo)\n",
    "    \n",
    "    U.log('Saving the final results.')\n",
    "    bundle.save(result.models, f'models_{algo}_{version}')\n",
    "    bundle.save(opt_bounds, 'bounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not U.on_kaggle():\n",
    "    submit(inference(X_tst, model=algo, version=version))\n",
    "    assert os.path.exists('submission.csv')\n",
    "    assert pd.read_csv('submission.csv').shape[0] == 1000\n",
    "    bundle.package(folder='/home/ck/data/bowl2019/external/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
