{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: xx_extract_features.ipynb -> ../extract_features.py\r\n",
      "1 notebook(s) exported into folder: ..\r\n"
     ]
    }
   ],
   "source": [
    "!python -m jupytools export -nb \"xx_extract_features.ipynb\" -o .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupytools.syspath\n",
    "jupytools.syspath.add('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from utils import parallel, parallel_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Data Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEFAULT_EVENT_FEATUERS = (\n",
    "    'game_time', 'coordinates.y', 'coordinates.stage_height',\n",
    "    'coordinates.stage_width', 'coordinates.x',\n",
    "    'description', 'media_type', 'identifier',\n",
    "    'duration', 'total_duration'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EventParser:\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "    def __call__(self, json_str):\n",
    "        obj = json.loads(json_str)\n",
    "        obj = pd.io.json.json_normalize(obj)\n",
    "        obj = obj.T[0].to_dict()\n",
    "        row = {k: obj.get(k) for k in self.keys}\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fillna(df, column, method='mean', value=None, fallback=0):\n",
    "    if df[column].isna().all():\n",
    "        value = fallback\n",
    "    elif method == 'mean':\n",
    "        value = df[column].mean()\n",
    "    elif method == 'mode':\n",
    "        value = df[column].value_counts().index[0]\n",
    "    elif method == 'const':\n",
    "        assert value is not None\n",
    "    else:\n",
    "        raise RuntimeError(f'invalid imputing method: {method}')\n",
    "    df[column] = df[column].fillna(value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_event_data(df, features=DEFAULT_EVENT_FEATUERS,\n",
    "                       num_workers=cpu_count(), pbar=False,\n",
    "                       **opts):\n",
    "    \n",
    "    parse_row = EventParser(features)\n",
    "    event_data = df.event_data\n",
    "    if pbar:\n",
    "        event_data = tqdm(event_data, desc='Processing events')\n",
    "    df = pd.DataFrame(parallel(parse_row, event_data, num_workers))\n",
    "    df = fillna(df, 'game_time', method='mean')\n",
    "    df = fillna(df, 'coordinates.x', method='mode')\n",
    "    df = fillna(df, 'coordinates.y', method='mode')\n",
    "    df = fillna(df, 'coordinates.stage_height', method='mode')\n",
    "    df = fillna(df, 'coordinates.stage_width', method='mode')\n",
    "    df = fillna(df, 'description', method='mode', fallback='none')\n",
    "    df = fillna(df, 'media_type', method='const', value='none')\n",
    "    df = fillna(df, 'identifier', method='const', value='none')\n",
    "    df = fillna(df, 'duration', method='mean')\n",
    "    df = fillna(df, 'total_duration', method='mean')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extend_with_event_data(dataset, **opts):\n",
    "    events = extract_event_data(dataset, **opts)\n",
    "    joined = pd.concat([\n",
    "        dataset.drop(columns='event_data'),\n",
    "        events.drop(columns='game_time')\n",
    "    ], axis=1)\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def basic_features(group):\n",
    "    row = OrderedDict()\n",
    "    row['game_session'] = group.game_session.iloc[-1]\n",
    "    row['installation_id'] = group.installation_id.iloc[-1]\n",
    "    row['events_count'] = len(group)\n",
    "    row['game_session_uniq'] = len(group.game_session.unique())\n",
    "    row['title_uniq'] = len(group.title.unique())\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def event_data_features(group):\n",
    "    event_cnt = group.event_code.value_counts()\n",
    "    row = OrderedDict()\n",
    "    row['event_count_max'] = group.event_count.max()\n",
    "    row['event_code_least'] = event_cnt.index[-1]\n",
    "    row['event_code_most'] = event_cnt.index[0]\n",
    "    row['event_code_uniq'] = len(event_cnt)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def game_time_features(group):\n",
    "    row = OrderedDict()\n",
    "    gt = group.game_time\n",
    "    row['game_time_min'] = gt.min()\n",
    "    row['game_time_max'] = gt.max()\n",
    "    row['game_time_mean'] = gt.mean()\n",
    "    row['game_time_std'] = gt.std()\n",
    "    row['game_time_skew'] = gt.skew()\n",
    "    row['game_time_sum'] = gt.sum()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def type_features(group):\n",
    "    n = len(group)\n",
    "    type_cnt = group.type.value_counts()\n",
    "    row = OrderedDict()\n",
    "    row['type_uniq'] = len(type_cnt)\n",
    "    row['type_least'] = type_cnt.index[-1]\n",
    "    row['type_most'] = type_cnt.index[0]\n",
    "    row['type_clip_count'] = type_cnt.get('Clip', 0)\n",
    "    row['type_activity_count'] = type_cnt.get('Activity', 0)\n",
    "    row['type_game_count'] = type_cnt.get('Game', 0)\n",
    "    row['type_assessment_count'] = type_cnt.get('Assessment', 0)\n",
    "    row['type_clip_freq'] = row['type_clip_count']/n\n",
    "    row['type_activity_freq'] = row['type_activity_count']/n\n",
    "    row['type_game_freq'] = row['type_game_count']/n\n",
    "    row['type_assessment_freq'] = row['type_assessment_count']/n\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def world_features(group):\n",
    "    n = len(group)\n",
    "    world_cnt = group.world.value_counts()\n",
    "    row = OrderedDict()\n",
    "    row['world_uniq'] = len(world_cnt)\n",
    "    row['world_least'] = world_cnt.index[-1]\n",
    "    row['world_most'] = world_cnt.index[0]\n",
    "    row['world_none_count'] = world_cnt.get('NONE', 0)\n",
    "    row['world_magmapeak_count'] = world_cnt.get('MAGMAPEAK', 0)\n",
    "    row['world_treetopcity_count'] = world_cnt.get('TREETOPCITY', 0)\n",
    "    row['world_crystalcaves_count'] = world_cnt.get('CRYSTALCAVES', 0)\n",
    "    row['world_none_freq'] = row['world_none_count']/n\n",
    "    row['world_magmapeak_freq'] = row['world_magmapeak_count']/n\n",
    "    row['world_treetopcity_freq'] = row['world_treetopcity_count']/n\n",
    "    row['world_crystalcaves_freq'] = row['world_crystalcaves_count']/n\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def media_features(group):\n",
    "    n = len(group)\n",
    "    media_cnt = group.media_type.value_counts()\n",
    "    row = OrderedDict()\n",
    "    row['media_uniq'] = len(media_cnt)\n",
    "    row['media_least'] = media_cnt.index[-1]\n",
    "    row['media_most'] = media_cnt.index[0]\n",
    "    row['media_none_count'] = media_cnt.get('none', 0)\n",
    "    row['media_audio_count'] = media_cnt.get('audio', 0)\n",
    "    row['media_animation_count'] = media_cnt.get('animation', 0)\n",
    "    row['media_none_freq'] = row['media_none_count']/n\n",
    "    row['media_audio_freq'] = row['media_audio_count']/n\n",
    "    row['media_animation_freq'] = row['media_animation_count']/n\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def timestamp_features(group):\n",
    "    ts = group['timestamp']\n",
    "    row = OrderedDict()\n",
    "    \n",
    "    row['ts_month_first'] = ts.dt.month.min()\n",
    "    row['ts_month_last'] = ts.dt.month.max()\n",
    "    row['ts_month_mean'] = ts.dt.month.mean()\n",
    "    row['ts_month_std'] = ts.dt.month.std()\n",
    "    row['ts_month_diff'] = row['ts_month_last'] - row['ts_month_first']\n",
    "    row['ts_month_uniq'] = len(ts.dt.month.unique())\n",
    "    \n",
    "    row['ts_day_first'] = ts.dt.day.min()\n",
    "    row['ts_day_last'] = ts.dt.day.max()\n",
    "    row['ts_day_mean'] = ts.dt.day.mean()\n",
    "    row['ts_day_std'] = ts.dt.day.std()\n",
    "    row['ts_day_diff'] = row['ts_day_last'] - row['ts_day_first']\n",
    "    row['ts_day_uniq'] = len(ts.dt.month.unique())\n",
    "    \n",
    "    dow_cnt = ts.dt.dayofweek.value_counts()\n",
    "    row['ts_dow_least'] = dow_cnt.index[-1]\n",
    "    row['ts_dow_most'] = dow_cnt.index[0]\n",
    "    row['ts_dow_mean'] = ts.dt.dayofweek.mean()\n",
    "    row['ts_dow_std'] = ts.dt.dayofweek.std()\n",
    "    \n",
    "    hour_cnt = ts.dt.hour.value_counts()\n",
    "    row['ts_hour_least'] = hour_cnt.index[-1]\n",
    "    row['ts_hour_most'] = hour_cnt.index[0]\n",
    "    row['ts_hour_mean'] = ts.dt.hour.mean()\n",
    "    row['ts_hour_std'] = ts.dt.hour.std()\n",
    "    \n",
    "    delta = ts.max() - ts.min()\n",
    "    row['ts_delta_seconds'] = delta.total_seconds()\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "SMOKE_TEST_FEATURES = [\n",
    "    basic_features,\n",
    "    event_data_features,\n",
    "    game_time_features,\n",
    "    type_features,\n",
    "    world_features,\n",
    "    media_features\n",
    "]\n",
    "\n",
    "class FeaturesExtractor:\n",
    "    def __call__(self, dataset, *args, **kwargs):\n",
    "        return self.extract(dataset, *args, **kwargs)\n",
    "    def extract(self, dataset):\n",
    "        raise NotImplemetedError()\n",
    "        \n",
    "class AssessmentFeatures(FeaturesExtractor):\n",
    "    def __init__(self, steps):\n",
    "        super().__init__()\n",
    "        self.steps = steps\n",
    "    def extract(self, group):\n",
    "        g = group.reset_index(drop=True)\n",
    "        assessments = g.index[g.title.str.contains('\\(Assessment\\)')].tolist()\n",
    "        rows = []\n",
    "        for i, line_no in enumerate(assessments):\n",
    "            asm = g.iloc[:line_no+1].copy()\n",
    "            row = OrderedDict()\n",
    "            for step in self.steps:\n",
    "                row.update(step(asm))\n",
    "            rows.append(row)\n",
    "        return rows\n",
    "    \n",
    "class BaselineFeatures(AssessmentFeatures):\n",
    "    def __init__(self):\n",
    "        super().__init__(steps=SMOKE_TEST_FEATURES)\n",
    "        \n",
    "class BaselineWithTimestampFeatures(AssessmentFeatures):\n",
    "    def __init__(self):\n",
    "        super().__init__(steps=SMOKE_TEST_FEATURES + [timestamp_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def baseline_features(group):\n",
    "    g = group.reset_index(drop=True)\n",
    "    \n",
    "    assessments = g.index[g.title.str.contains('\\(Assessment\\)')].tolist()\n",
    "    \n",
    "    steps = [\n",
    "        basic_features,\n",
    "        event_data_features,\n",
    "        game_time_features,\n",
    "        type_features,\n",
    "        world_features,\n",
    "        media_features\n",
    "    ]\n",
    "    \n",
    "    rows = []\n",
    "    for i, line_no in enumerate(assessments):\n",
    "        asm = g.iloc[:line_no+1].copy()\n",
    "        row = OrderedDict()\n",
    "        for step in steps:\n",
    "            row.update(step(asm))\n",
    "        rows.append(row)\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Extractor Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_with_groups(groups, prepare_fn, num_workers=cpu_count(), targets=None, **opts):\n",
    "    dataset = pd.DataFrame(parallel_chain(prepare_fn, groups, num_workers))\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    if targets is not None:\n",
    "        dataset = dataset[dataset.game_session.isin(targets.game_session)]\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_with_events(dataset, prepare_fn, pbar=False, **opts):\n",
    "    grouped = dataset.groupby('installation_id')\n",
    "    total = grouped.ngroups\n",
    "    groups = (g for _, g in grouped)\n",
    "    if pbar:\n",
    "        groups = tqdm(groups, total=total, desc='Preparing group') \n",
    "    return prepare_with_groups(groups, prepare_fn, **opts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare(dataset, prepare_fn, **opts):\n",
    "    dataset = extend_with_event_data(dataset, **opts)\n",
    "    dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
    "    return prepare_with_events(dataset, prepare_fn, **opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai (cuda 10)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
