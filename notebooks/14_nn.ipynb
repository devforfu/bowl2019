{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /kaggle/input/data-bowl-2019-external-data/*.py /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import jupytools.syspath\n",
    "def ignore(*args, **kwargs): pass\n",
    "warnings.warn = ignore\n",
    "jupytools.syspath.add('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as models\n",
    "\n",
    "import bundle\n",
    "import features as F\n",
    "import selection\n",
    "import utils as U\n",
    "from dataset import load, load_sample, Subset\n",
    "from encode import encode\n",
    "from training import train, inference, submit, EnsembleTrainer, get_default_config\n",
    "from meta import compute_meta_data\n",
    "from metric import optimize_rounding_bounds, make_cappa_metric, round_regressor_predictions\n",
    "from normalize import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train and test.\n",
      "(11341042, 11) (17690, 7) (386, 3) (1156414, 11) "
     ]
    }
   ],
   "source": [
    "sample = False\n",
    "if U.on_kaggle():\n",
    "    U.log('Loading test set only.')\n",
    "    tst_data = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "else:\n",
    "    if sample:\n",
    "        U.log('Warning: loading train and test data sample.')\n",
    "        trn_data, _, _ = load_sample(Subset.Train, 500_000)\n",
    "        [tst_data] = load_sample(Subset.Test, 500_000)\n",
    "    else:\n",
    "        U.log('Loading train and test.')\n",
    "        trn_data, trn_spec, trn_targ = load(Subset.Train)\n",
    "        [tst_data] = load(Subset.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming train and test data.\n",
      "(11341042, 19) (1156414, 19)\n"
     ]
    }
   ],
   "source": [
    "transform = U.combine(\n",
    "    partial(F.add_feature_combinations, pairs=[('title', 'event_code')]),\n",
    "    partial(F.add_datetime, column='timestamp', prefix='ts'),\n",
    ")\n",
    "\n",
    "if U.on_kaggle():\n",
    "    U.log('Transforming test data only.')\n",
    "    X_tst = transform(tst_data.copy())\n",
    "    U.log(X_tst.shape)\n",
    "else:\n",
    "    U.log('Transforming train and test data.')\n",
    "    X_tst = transform(tst_data.copy())\n",
    "    X_trn = transform(trn_data.copy())\n",
    "    U.log(X_trn.shape, X_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing meta using train and test datasets.\n",
      "Saving computed meta on disk.\n"
     ]
    }
   ],
   "source": [
    "if U.on_kaggle():\n",
    "    U.log('Reading pre-computed meta from disk.')\n",
    "    meta = bundle.meta()\n",
    "else:\n",
    "    U.log('Computing meta using train and test datasets.')\n",
    "    meta = compute_meta_data(X_trn, X_tst)\n",
    "    U.log('Saving computed meta on disk.')\n",
    "    bundle.save_meta(meta, 'meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing train and test datasets.\n",
      "Running algorithm in train mode.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0004cc6c17e142c0a766974baff46ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running algorithm in test mode.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a378376550244c9b868e30e1a04f014c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extractor = F.FeaturesExtractor([\n",
    "    F.CountingFeatures(meta),\n",
    "    F.PerformanceFeatures(meta),\n",
    "    F.VarietyFeatures(meta),\n",
    "    F.EventDataFeatures(meta),\n",
    "    F.FeedbackFeatures(meta)\n",
    "])\n",
    "\n",
    "algo = F.InMemoryAlgorithm(extractor, meta, num_workers=12)\n",
    "\n",
    "cat_cols = ['session_title']\n",
    "\n",
    "if U.on_kaggle():\n",
    "    U.log('Preparing test dataset.')\n",
    "    X_tst = algo.run(X_tst, test=True)\n",
    "    encoders = bundle.encoders()\n",
    "    X_tst, _ = encode(X_tst, cat_cols, encoders=encoders)\n",
    "else:\n",
    "    U.log('Preparing train and test datasets.')\n",
    "    X_trn = algo.run(X_trn)\n",
    "    X_tst = algo.run(X_tst, test=True)\n",
    "    X_trn, encoders = encode(X_trn, cat_cols)\n",
    "    X_tst, _ = encode(X_tst, cat_cols, encoders=encoders)\n",
    "    bundle.save(encoders, 'encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running post-processing on train and test sets.\n"
     ]
    }
   ],
   "source": [
    "if U.on_kaggle():\n",
    "    U.log('Running post-processing on test set only.')\n",
    "    F.add_user_wise_features(X_tst, meta)\n",
    "else:\n",
    "    U.log('Running post-processing on train and test sets.')\n",
    "    F.add_user_wise_features(X_trn, meta)\n",
    "    F.add_user_wise_features(X_tst, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting relevant features.\n",
      "Excluding from consideration: ['installation_id', 'game_session', 'accuracy_group']\n",
      "Applying feature selection rule: nonzero\n",
      "Selected features: 987 of 1020\n",
      "Keeping only features, selected by every rule.\n",
      "Final number of features changed from 1020 to 987\n"
     ]
    }
   ],
   "source": [
    "non_train_cols = ['installation_id', 'game_session', 'accuracy_group']\n",
    "if U.on_kaggle():\n",
    "    U.log('Loading features from disk.')\n",
    "    features = bundle.features()\n",
    "else:\n",
    "    U.log('Selecting relevant features.')\n",
    "    selector = selection.FeatureSelection(\n",
    "        rules=[('nonzero', selection.non_zero_rows_and_cols)], \n",
    "        ignore_cols=non_train_cols)\n",
    "    features = selector.select(X_trn)\n",
    "    bundle.save(features, 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not U.on_kaggle():\n",
    "    X_trn.to_pickle('/tmp/nn/X_trn.pickle')\n",
    "    X_tst.to_pickle('/tmp/nn/X_tst.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    K = tf.keras.backend\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def build_model(num_cols, cat_cols, cat_sizes, \n",
    "                output_size, output_act, loss):\n",
    "    \n",
    "    def prepare_input(data):\n",
    "        return [data[num_cols]] + [data[col].T for col in cat_cols]\n",
    "    \n",
    "    def numerical(input_size):\n",
    "        i = L.Input(shape=(input_size,))\n",
    "        x = L.Dense(2048, activation='relu', use_bias=False)(i)\n",
    "        x = L.BatchNormalization()(x)\n",
    "        x = L.Dropout(0.5)(x)\n",
    "        m = models.Model(inputs=i, outputs=x)\n",
    "        return m\n",
    "    \n",
    "    def categorical(cat_sizes):\n",
    "        inputs, embeds = [], []\n",
    "        for cat_size in cat_sizes:\n",
    "            emb_sz = min(50, cat_size // 2)\n",
    "            i = L.Input(shape=(1,))\n",
    "            x = L.Embedding(output_dim=emb_sz, input_dim=cat_size)(i)\n",
    "            inputs.append(i)\n",
    "            embeds.append(x)\n",
    "        if len(embeds) > 1:\n",
    "            x = L.concatenate(embeds)\n",
    "        x = L.Flatten()(x)\n",
    "        m = models.Model(inputs=inputs, outputs=x)\n",
    "        return m\n",
    "    \n",
    "    with tf.device('/GPU:1'):\n",
    "        num = numerical(len(num_cols))\n",
    "        cat = categorical(cat_sizes)\n",
    "        x = L.concatenate(num.outputs + cat.outputs)\n",
    "        x = L.Dense(1024, activation='relu', use_bias=False)(x)\n",
    "        x = L.BatchNormalization()(x)\n",
    "        x = L.Dropout(0.25)(x)\n",
    "        x = L.Dense(512, activation='relu', use_bias=False)(x)\n",
    "        x = L.BatchNormalization()(x)\n",
    "        x = L.Dropout(0.25)(x)\n",
    "        x = L.Dense(256, activation='relu', use_bias=False)(x)\n",
    "        x = L.BatchNormalization()(x)\n",
    "        x = L.Dropout(0.25)(x)\n",
    "        x = L.Dense(128, activation='relu', use_bias=False)(x)\n",
    "        x = L.BatchNormalization()(x)\n",
    "        x = L.Dropout(0.25)(x)\n",
    "        x = L.Dense(output_size, activation=output_act)(x)\n",
    "        model = models.Model(inputs=num.inputs + cat.inputs, outputs=x)\n",
    "        opt = tf.keras.optimizer\n",
    "        model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "        \n",
    "    return model, prepare_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalize numerical columns.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fa173057e543cd9a744ad218136eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=986), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not U.on_kaggle():\n",
    "    dataset = pd.read_pickle('/tmp/nn/X_trn.pickle')\n",
    "    target_col = 'accuracy_group'\n",
    "    cat_cols = ['session_title']\n",
    "    num_cols = [f for f in features if f not in cat_cols]\n",
    "    U.log('Normalize numerical columns.')\n",
    "    normalize(dataset, num_cols, grouping_key='session_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model folds.\n",
      "Fold #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:24:03.007883 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14152/14152 [==============================] - 1s 78us/sample - loss: 3.6413 - val_loss: 1.9627\n",
      "Epoch 2/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.2878 - val_loss: 1.8822\n",
      "Epoch 3/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 2.0975 - val_loss: 1.8318\n",
      "Epoch 4/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.9987 - val_loss: 1.8183\n",
      "Epoch 5/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.9222 - val_loss: 1.6259\n",
      "Epoch 6/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.7975 - val_loss: 1.3975\n",
      "Epoch 7/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.7532 - val_loss: 1.4145\n",
      "Epoch 8/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.6879 - val_loss: 1.3551\n",
      "Epoch 9/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.5986 - val_loss: 1.2627\n",
      "Epoch 10/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.5315 - val_loss: 1.2788\n",
      "Epoch 11/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.5149 - val_loss: 1.2248\n",
      "Epoch 12/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4388 - val_loss: 1.2070\n",
      "Epoch 13/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3818 - val_loss: 1.1978\n",
      "Epoch 14/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3739 - val_loss: 1.1983\n",
      "Epoch 15/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3282 - val_loss: 1.1906\n",
      "Epoch 16/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3321 - val_loss: 1.1811\n",
      "Epoch 17/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2723 - val_loss: 1.1717\n",
      "Epoch 18/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2889 - val_loss: 1.2708\n",
      "Epoch 19/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3066 - val_loss: 1.2500\n",
      "Epoch 20/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2581 - val_loss: 1.1819\n",
      "Epoch 21/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2539 - val_loss: 1.1432\n",
      "Epoch 22/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2446 - val_loss: 1.1696\n",
      "Epoch 23/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2291 - val_loss: 1.2695\n",
      "Epoch 24/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2290 - val_loss: 1.1559\n",
      "Epoch 25/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1770 - val_loss: 1.1485\n",
      "Epoch 26/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2026 - val_loss: 1.1920\n",
      "Epoch 27/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1910 - val_loss: 1.1559\n",
      "Epoch 28/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1718 - val_loss: 1.1453\n",
      "Epoch 29/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1427 - val_loss: 1.1206\n",
      "Epoch 30/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1501 - val_loss: 1.0898\n",
      "Epoch 31/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1556 - val_loss: 1.1090\n",
      "Epoch 32/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1101 - val_loss: 1.1383\n",
      "Epoch 33/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1009 - val_loss: 1.0857\n",
      "Epoch 34/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1053 - val_loss: 1.1166\n",
      "Epoch 35/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1161 - val_loss: 1.1814\n",
      "Epoch 36/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0748 - val_loss: 1.0802\n",
      "Epoch 37/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0788 - val_loss: 1.1588\n",
      "Epoch 38/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1119 - val_loss: 1.0887\n",
      "Epoch 39/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0556 - val_loss: 1.1705\n",
      "Epoch 40/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0584 - val_loss: 1.1431\n",
      "Epoch 41/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0401 - val_loss: 1.0874\n",
      "Epoch 42/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0328 - val_loss: 1.0732\n",
      "Epoch 43/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0160 - val_loss: 1.0728\n",
      "Epoch 44/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0539 - val_loss: 1.0884\n",
      "Epoch 45/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0089 - val_loss: 1.1218\n",
      "Epoch 46/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0056 - val_loss: 1.1615\n",
      "Epoch 47/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0023 - val_loss: 1.1122\n",
      "Epoch 48/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9941 - val_loss: 1.1267\n",
      "Epoch 49/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9936 - val_loss: 1.0698\n",
      "Epoch 50/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9718 - val_loss: 1.0620\n",
      "Epoch 51/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9494 - val_loss: 1.0524\n",
      "Epoch 52/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9505 - val_loss: 1.0614\n",
      "Epoch 53/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9682 - val_loss: 1.0953\n",
      "Epoch 54/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9497 - val_loss: 1.0991\n",
      "Epoch 55/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9178 - val_loss: 1.0625\n",
      "Epoch 56/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9412 - val_loss: 1.0775\n",
      "Epoch 57/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9355 - val_loss: 1.0921\n",
      "Epoch 58/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9046 - val_loss: 1.1307\n",
      "Epoch 59/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9080 - val_loss: 1.0578\n",
      "Epoch 60/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8974 - val_loss: 1.1025\n",
      "Epoch 61/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8859 - val_loss: 1.1332\n",
      "Epoch 62/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9081 - val_loss: 1.0777\n",
      "Epoch 63/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9256 - val_loss: 1.1015\n",
      "Epoch 64/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8769 - val_loss: 1.0817\n",
      "Epoch 65/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9003 - val_loss: 1.0921\n",
      "Epoch 66/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9070 - val_loss: 1.0693\n",
      "Epoch 67/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8527 - val_loss: 1.1376\n",
      "Epoch 68/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8891 - val_loss: 1.0938\n",
      "Epoch 69/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8719 - val_loss: 1.1057\n",
      "Epoch 70/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8444 - val_loss: 1.1461\n",
      "Epoch 71/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8293 - val_loss: 1.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:24:35.194547 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:24:36.710418 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14152/14152 [==============================] - 1s 76us/sample - loss: 3.8575 - val_loss: 1.9787\n",
      "Epoch 2/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.4075 - val_loss: 2.1277\n",
      "Epoch 3/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.0880 - val_loss: 2.0645\n",
      "Epoch 4/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.9982 - val_loss: 1.7040\n",
      "Epoch 5/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.0060 - val_loss: 1.6261\n",
      "Epoch 6/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.8669 - val_loss: 1.6496\n",
      "Epoch 7/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.7025 - val_loss: 1.5129\n",
      "Epoch 8/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.7205 - val_loss: 1.4447\n",
      "Epoch 9/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.6309 - val_loss: 1.3736\n",
      "Epoch 10/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5407 - val_loss: 1.5867\n",
      "Epoch 11/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5099 - val_loss: 1.3197\n",
      "Epoch 12/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4706 - val_loss: 1.2504\n",
      "Epoch 13/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3932 - val_loss: 1.3493\n",
      "Epoch 14/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3745 - val_loss: 1.3105\n",
      "Epoch 15/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3501 - val_loss: 1.2119\n",
      "Epoch 16/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3533 - val_loss: 1.1856\n",
      "Epoch 17/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3089 - val_loss: 1.2468\n",
      "Epoch 18/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3030 - val_loss: 1.2515\n",
      "Epoch 19/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2821 - val_loss: 1.2162\n",
      "Epoch 20/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2902 - val_loss: 1.2298\n",
      "Epoch 21/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2507 - val_loss: 1.1666\n",
      "Epoch 22/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2656 - val_loss: 1.2613\n",
      "Epoch 23/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2328 - val_loss: 1.1497\n",
      "Epoch 24/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2411 - val_loss: 1.2560\n",
      "Epoch 25/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2115 - val_loss: 1.1503\n",
      "Epoch 26/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1899 - val_loss: 1.1805\n",
      "Epoch 27/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1776 - val_loss: 1.1250\n",
      "Epoch 28/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1863 - val_loss: 1.1856\n",
      "Epoch 29/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1817 - val_loss: 1.1872\n",
      "Epoch 30/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1671 - val_loss: 1.1298\n",
      "Epoch 31/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1671 - val_loss: 1.1867\n",
      "Epoch 32/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1514 - val_loss: 1.1284\n",
      "Epoch 33/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1064 - val_loss: 1.1032\n",
      "Epoch 34/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1070 - val_loss: 1.1549\n",
      "Epoch 35/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1210 - val_loss: 1.1746\n",
      "Epoch 36/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0929 - val_loss: 1.1229\n",
      "Epoch 37/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0965 - val_loss: 1.0869\n",
      "Epoch 38/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0870 - val_loss: 1.1157\n",
      "Epoch 39/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0915 - val_loss: 1.1209\n",
      "Epoch 40/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0541 - val_loss: 1.0814\n",
      "Epoch 41/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0511 - val_loss: 1.1119\n",
      "Epoch 42/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0375 - val_loss: 1.0991\n",
      "Epoch 43/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0179 - val_loss: 1.1040\n",
      "Epoch 44/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0525 - val_loss: 1.0621\n",
      "Epoch 45/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0390 - val_loss: 1.1461\n",
      "Epoch 46/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0097 - val_loss: 1.0818\n",
      "Epoch 47/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0084 - val_loss: 1.0782\n",
      "Epoch 48/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9929 - val_loss: 1.1266\n",
      "Epoch 49/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9897 - val_loss: 1.1141\n",
      "Epoch 50/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9660 - val_loss: 1.0652\n",
      "Epoch 51/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9666 - val_loss: 1.0905\n",
      "Epoch 52/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9489 - val_loss: 1.2463\n",
      "Epoch 53/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9510 - val_loss: 1.1635\n",
      "Epoch 54/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9346 - val_loss: 1.0823\n",
      "Epoch 55/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9391 - val_loss: 1.0933\n",
      "Epoch 56/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9282 - val_loss: 1.1251\n",
      "Epoch 57/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9189 - val_loss: 1.0856\n",
      "Epoch 58/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8998 - val_loss: 1.1065\n",
      "Epoch 59/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9418 - val_loss: 1.0891\n",
      "Epoch 60/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9018 - val_loss: 1.2164\n",
      "Epoch 61/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9064 - val_loss: 1.1400\n",
      "Epoch 62/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8917 - val_loss: 1.1036\n",
      "Epoch 63/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8729 - val_loss: 1.1135\n",
      "Epoch 64/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8690 - val_loss: 1.1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:25:06.446625 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:25:07.763502 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14152/14152 [==============================] - 1s 75us/sample - loss: 3.7127 - val_loss: 2.1834\n",
      "Epoch 2/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 2.2691 - val_loss: 2.0240\n",
      "Epoch 3/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.0998 - val_loss: 1.8023\n",
      "Epoch 4/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 2.0096 - val_loss: 1.7329\n",
      "Epoch 5/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.9370 - val_loss: 1.6335\n",
      "Epoch 6/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.8455 - val_loss: 1.5340\n",
      "Epoch 7/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.7835 - val_loss: 1.4833\n",
      "Epoch 8/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.6522 - val_loss: 1.4496\n",
      "Epoch 9/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.6733 - val_loss: 1.3536\n",
      "Epoch 10/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5281 - val_loss: 1.3025\n",
      "Epoch 11/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5138 - val_loss: 1.4099\n",
      "Epoch 12/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4425 - val_loss: 1.2971\n",
      "Epoch 13/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4148 - val_loss: 1.2096\n",
      "Epoch 14/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3980 - val_loss: 1.2306\n",
      "Epoch 15/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3436 - val_loss: 1.2176\n",
      "Epoch 16/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3365 - val_loss: 1.1890\n",
      "Epoch 17/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3082 - val_loss: 1.2651\n",
      "Epoch 18/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2796 - val_loss: 1.1689\n",
      "Epoch 19/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2821 - val_loss: 1.1600\n",
      "Epoch 20/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2647 - val_loss: 1.1497\n",
      "Epoch 21/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2839 - val_loss: 1.1543\n",
      "Epoch 22/100\n",
      "14152/14152 [==============================] - 0s 31us/sample - loss: 1.2289 - val_loss: 1.1683\n",
      "Epoch 23/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2007 - val_loss: 1.1464\n",
      "Epoch 24/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2331 - val_loss: 1.1323\n",
      "Epoch 25/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1951 - val_loss: 1.2185\n",
      "Epoch 26/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1921 - val_loss: 1.1116\n",
      "Epoch 27/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1773 - val_loss: 1.2174\n",
      "Epoch 28/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1608 - val_loss: 1.2275\n",
      "Epoch 29/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1405 - val_loss: 1.2016\n",
      "Epoch 30/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1663 - val_loss: 1.1893\n",
      "Epoch 31/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1422 - val_loss: 1.1349\n",
      "Epoch 32/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1108 - val_loss: 1.2085\n",
      "Epoch 33/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1162 - val_loss: 1.0883\n",
      "Epoch 34/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1069 - val_loss: 1.1100\n",
      "Epoch 35/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1118 - val_loss: 1.1120\n",
      "Epoch 36/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0872 - val_loss: 1.1021\n",
      "Epoch 37/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0647 - val_loss: 1.1763\n",
      "Epoch 38/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0693 - val_loss: 1.1346\n",
      "Epoch 39/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0490 - val_loss: 1.1011\n",
      "Epoch 40/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0409 - val_loss: 1.1558\n",
      "Epoch 41/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0327 - val_loss: 1.1094\n",
      "Epoch 42/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0126 - val_loss: 1.0817\n",
      "Epoch 43/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0096 - val_loss: 1.0888\n",
      "Epoch 44/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0158 - val_loss: 1.1572\n",
      "Epoch 45/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0096 - val_loss: 1.1166\n",
      "Epoch 46/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9986 - val_loss: 1.1241\n",
      "Epoch 47/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9874 - val_loss: 1.0821\n",
      "Epoch 48/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9856 - val_loss: 1.1180\n",
      "Epoch 49/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9608 - val_loss: 1.0764\n",
      "Epoch 50/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9570 - val_loss: 1.0783\n",
      "Epoch 51/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9498 - val_loss: 1.0880\n",
      "Epoch 52/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9471 - val_loss: 1.0865\n",
      "Epoch 53/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9457 - val_loss: 1.1470\n",
      "Epoch 54/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9195 - val_loss: 1.0671\n",
      "Epoch 55/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9309 - val_loss: 1.0901\n",
      "Epoch 56/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9057 - val_loss: 1.1191\n",
      "Epoch 57/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9202 - val_loss: 1.0985\n",
      "Epoch 58/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9047 - val_loss: 1.0894\n",
      "Epoch 59/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9008 - val_loss: 1.0945\n",
      "Epoch 60/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8829 - val_loss: 1.1070\n",
      "Epoch 61/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8791 - val_loss: 1.1413\n",
      "Epoch 62/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8869 - val_loss: 1.0943\n",
      "Epoch 63/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8827 - val_loss: 1.0587\n",
      "Epoch 64/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8487 - val_loss: 1.0837\n",
      "Epoch 65/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8643 - val_loss: 1.1318\n",
      "Epoch 66/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8686 - val_loss: 1.1005\n",
      "Epoch 67/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8380 - val_loss: 1.0847\n",
      "Epoch 68/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8459 - val_loss: 1.0875\n",
      "Epoch 69/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8423 - val_loss: 1.1088\n",
      "Epoch 70/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8309 - val_loss: 1.1034\n",
      "Epoch 71/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8212 - val_loss: 1.1601\n",
      "Epoch 72/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8370 - val_loss: 1.1278\n",
      "Epoch 73/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8096 - val_loss: 1.1654\n",
      "Epoch 74/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8124 - val_loss: 1.1465\n",
      "Epoch 75/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8052 - val_loss: 1.1158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8008 - val_loss: 1.1047\n",
      "Epoch 77/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8096 - val_loss: 1.1293\n",
      "Epoch 78/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.7921 - val_loss: 1.1715\n",
      "Epoch 79/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.7968 - val_loss: 1.1679\n",
      "Epoch 80/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.7792 - val_loss: 1.1516\n",
      "Epoch 81/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.7822 - val_loss: 1.1338\n",
      "Epoch 82/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.7869 - val_loss: 1.1522\n",
      "Epoch 83/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.7648 - val_loss: 1.1298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:25:46.014004 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:25:47.347792 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14152/14152 [==============================] - 1s 76us/sample - loss: 3.8440 - val_loss: 2.0038\n",
      "Epoch 2/100\n",
      "14152/14152 [==============================] - 0s 28us/sample - loss: 2.2572 - val_loss: 1.8944\n",
      "Epoch 3/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.1116 - val_loss: 1.8092\n",
      "Epoch 4/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.9963 - val_loss: 1.6620\n",
      "Epoch 5/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 2.0094 - val_loss: 1.5852\n",
      "Epoch 6/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.8077 - val_loss: 1.6082\n",
      "Epoch 7/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.7898 - val_loss: 1.4699\n",
      "Epoch 8/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.6780 - val_loss: 1.3282\n",
      "Epoch 9/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.6200 - val_loss: 1.4237\n",
      "Epoch 10/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5587 - val_loss: 1.5350\n",
      "Epoch 11/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.5050 - val_loss: 1.2851\n",
      "Epoch 12/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.4808 - val_loss: 1.2375\n",
      "Epoch 13/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4076 - val_loss: 1.2486\n",
      "Epoch 14/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.4254 - val_loss: 1.2040\n",
      "Epoch 15/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3728 - val_loss: 1.2157\n",
      "Epoch 16/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.3669 - val_loss: 1.2146\n",
      "Epoch 17/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2974 - val_loss: 1.1933\n",
      "Epoch 18/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2907 - val_loss: 1.2004\n",
      "Epoch 19/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2987 - val_loss: 1.2334\n",
      "Epoch 20/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2385 - val_loss: 1.2366\n",
      "Epoch 21/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2790 - val_loss: 1.2600\n",
      "Epoch 22/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2818 - val_loss: 1.1857\n",
      "Epoch 23/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2280 - val_loss: 1.2631\n",
      "Epoch 24/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2288 - val_loss: 1.1762\n",
      "Epoch 25/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2134 - val_loss: 1.1494\n",
      "Epoch 26/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1972 - val_loss: 1.1614\n",
      "Epoch 27/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2021 - val_loss: 1.2091\n",
      "Epoch 28/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2308 - val_loss: 1.1601\n",
      "Epoch 29/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1634 - val_loss: 1.1281\n",
      "Epoch 30/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1521 - val_loss: 1.2582\n",
      "Epoch 31/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1454 - val_loss: 1.2096\n",
      "Epoch 32/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1163 - val_loss: 1.1424\n",
      "Epoch 33/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1140 - val_loss: 1.1732\n",
      "Epoch 34/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1173 - val_loss: 1.1404\n",
      "Epoch 35/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1039 - val_loss: 1.1576\n",
      "Epoch 36/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1523 - val_loss: 1.0954\n",
      "Epoch 37/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0989 - val_loss: 1.1170\n",
      "Epoch 38/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.1232 - val_loss: 1.1688\n",
      "Epoch 39/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0970 - val_loss: 1.1045\n",
      "Epoch 40/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0571 - val_loss: 1.1324\n",
      "Epoch 41/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0810 - val_loss: 1.1205\n",
      "Epoch 42/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0203 - val_loss: 1.1205\n",
      "Epoch 43/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0443 - val_loss: 1.0810\n",
      "Epoch 44/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0285 - val_loss: 1.1115\n",
      "Epoch 45/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0425 - val_loss: 1.0693\n",
      "Epoch 46/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0590 - val_loss: 1.1087\n",
      "Epoch 47/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0377 - val_loss: 1.1721\n",
      "Epoch 48/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0155 - val_loss: 1.1240\n",
      "Epoch 49/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.0078 - val_loss: 1.1314\n",
      "Epoch 50/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9865 - val_loss: 1.0912\n",
      "Epoch 51/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9631 - val_loss: 1.1329\n",
      "Epoch 52/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9827 - val_loss: 1.1291\n",
      "Epoch 53/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9887 - val_loss: 1.1652\n",
      "Epoch 54/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9614 - val_loss: 1.1475\n",
      "Epoch 55/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9701 - val_loss: 1.1390\n",
      "Epoch 56/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9494 - val_loss: 1.1223\n",
      "Epoch 57/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9342 - val_loss: 1.1549\n",
      "Epoch 58/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9029 - val_loss: 1.1000\n",
      "Epoch 59/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9574 - val_loss: 1.0935\n",
      "Epoch 60/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8834 - val_loss: 1.1187\n",
      "Epoch 61/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9119 - val_loss: 1.1612\n",
      "Epoch 62/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8796 - val_loss: 1.1295\n",
      "Epoch 63/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9056 - val_loss: 1.1276\n",
      "Epoch 64/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8858 - val_loss: 1.1224\n",
      "Epoch 65/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8813 - val_loss: 1.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:26:18.153213 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:26:19.482959 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14152 samples, validate on 3538 samples\n",
      "Epoch 1/100\n",
      "14152/14152 [==============================] - 1s 77us/sample - loss: 3.1089 - val_loss: 1.9792\n",
      "Epoch 2/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 2.3372 - val_loss: 1.8633\n",
      "Epoch 3/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 2.0613 - val_loss: 1.8495\n",
      "Epoch 4/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.9830 - val_loss: 1.9746\n",
      "Epoch 5/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.9125 - val_loss: 1.6615\n",
      "Epoch 6/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.7821 - val_loss: 1.5323\n",
      "Epoch 7/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.7015 - val_loss: 1.4303\n",
      "Epoch 8/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.6661 - val_loss: 1.3659\n",
      "Epoch 9/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5743 - val_loss: 1.2454\n",
      "Epoch 10/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.5452 - val_loss: 1.2431\n",
      "Epoch 11/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4587 - val_loss: 1.2882\n",
      "Epoch 12/100\n",
      "14152/14152 [==============================] - 0s 31us/sample - loss: 1.4536 - val_loss: 1.2372\n",
      "Epoch 13/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.4747 - val_loss: 1.2183\n",
      "Epoch 14/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3249 - val_loss: 1.2170\n",
      "Epoch 15/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3205 - val_loss: 1.2119\n",
      "Epoch 16/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3254 - val_loss: 1.2875\n",
      "Epoch 17/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2940 - val_loss: 1.2428\n",
      "Epoch 18/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.3275 - val_loss: 1.2340\n",
      "Epoch 19/100\n",
      "14152/14152 [==============================] - 0s 31us/sample - loss: 1.2934 - val_loss: 1.2160\n",
      "Epoch 20/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2640 - val_loss: 1.2502\n",
      "Epoch 21/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 1.2767 - val_loss: 1.2404\n",
      "Epoch 22/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2382 - val_loss: 1.2456\n",
      "Epoch 23/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2137 - val_loss: 1.2921\n",
      "Epoch 24/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.2493 - val_loss: 1.2457\n",
      "Epoch 25/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1845 - val_loss: 1.2425\n",
      "Epoch 26/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1979 - val_loss: 1.1687\n",
      "Epoch 27/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1875 - val_loss: 1.2610\n",
      "Epoch 28/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1632 - val_loss: 1.2641\n",
      "Epoch 29/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1474 - val_loss: 1.2116\n",
      "Epoch 30/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1712 - val_loss: 1.3088\n",
      "Epoch 31/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1311 - val_loss: 1.2682\n",
      "Epoch 32/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1457 - val_loss: 1.2384\n",
      "Epoch 33/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.1275 - val_loss: 1.2110\n",
      "Epoch 34/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0509 - val_loss: 1.3007\n",
      "Epoch 35/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0772 - val_loss: 1.2819\n",
      "Epoch 36/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0891 - val_loss: 1.2681\n",
      "Epoch 37/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0902 - val_loss: 1.2253\n",
      "Epoch 38/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0719 - val_loss: 1.2548\n",
      "Epoch 39/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0808 - val_loss: 1.2866\n",
      "Epoch 40/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0883 - val_loss: 1.1993\n",
      "Epoch 41/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0543 - val_loss: 1.2301\n",
      "Epoch 42/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0706 - val_loss: 1.2998\n",
      "Epoch 43/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0288 - val_loss: 1.1658\n",
      "Epoch 44/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0477 - val_loss: 1.2190\n",
      "Epoch 45/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0171 - val_loss: 1.2245\n",
      "Epoch 46/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0102 - val_loss: 1.2989\n",
      "Epoch 47/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0100 - val_loss: 1.2175\n",
      "Epoch 48/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0367 - val_loss: 1.1655\n",
      "Epoch 49/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9990 - val_loss: 1.1733\n",
      "Epoch 50/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9888 - val_loss: 1.2251\n",
      "Epoch 51/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0233 - val_loss: 1.1826\n",
      "Epoch 52/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9802 - val_loss: 1.2047\n",
      "Epoch 53/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9829 - val_loss: 1.2627\n",
      "Epoch 54/100\n",
      "14152/14152 [==============================] - 0s 31us/sample - loss: 0.9560 - val_loss: 1.1868\n",
      "Epoch 55/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 1.0265 - val_loss: 1.1934\n",
      "Epoch 56/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9736 - val_loss: 1.1678\n",
      "Epoch 57/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9716 - val_loss: 1.1982\n",
      "Epoch 58/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.9647 - val_loss: 1.1540\n",
      "Epoch 59/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9363 - val_loss: 1.1703\n",
      "Epoch 60/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9385 - val_loss: 1.1474\n",
      "Epoch 61/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9148 - val_loss: 1.1757\n",
      "Epoch 62/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9080 - val_loss: 1.1663\n",
      "Epoch 63/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9171 - val_loss: 1.1696\n",
      "Epoch 64/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8875 - val_loss: 1.1642\n",
      "Epoch 65/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9076 - val_loss: 1.1246\n",
      "Epoch 66/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8929 - val_loss: 1.2022\n",
      "Epoch 67/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9326 - val_loss: 1.1348\n",
      "Epoch 68/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8923 - val_loss: 1.1962\n",
      "Epoch 69/100\n",
      "14152/14152 [==============================] - 0s 31us/sample - loss: 0.8606 - val_loss: 1.1782\n",
      "Epoch 70/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8913 - val_loss: 1.2274\n",
      "Epoch 71/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8744 - val_loss: 1.2379\n",
      "Epoch 72/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8283 - val_loss: 1.1926\n",
      "Epoch 73/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.9012 - val_loss: 1.1609\n",
      "Epoch 74/100\n",
      "14152/14152 [==============================] - 0s 31us/sample - loss: 0.8474 - val_loss: 1.1836\n",
      "Epoch 75/100\n",
      "14152/14152 [==============================] - 0s 29us/sample - loss: 0.8472 - val_loss: 1.2092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8399 - val_loss: 1.2212\n",
      "Epoch 77/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8366 - val_loss: 1.2047\n",
      "Epoch 78/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8365 - val_loss: 1.1713\n",
      "Epoch 79/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8257 - val_loss: 1.2231\n",
      "Epoch 80/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8270 - val_loss: 1.1966\n",
      "Epoch 81/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8310 - val_loss: 1.1825\n",
      "Epoch 82/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8812 - val_loss: 1.2499\n",
      "Epoch 83/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8236 - val_loss: 1.1823\n",
      "Epoch 84/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.7915 - val_loss: 1.1878\n",
      "Epoch 85/100\n",
      "14152/14152 [==============================] - 0s 30us/sample - loss: 0.8089 - val_loss: 1.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:26:59.875665 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "if not U.on_kaggle():\n",
    "    U.log('Training model folds.')\n",
    "    folds = GroupKFold(n_splits=5)\n",
    "    dataset = dataset.sample(dataset.shape[0])\n",
    "    group = dataset['installation_id']\n",
    "    X, y = dataset[features], dataset[target_col]\n",
    "    oof = np.zeros(len(y), dtype=np.float32)\n",
    "    cappa = make_cappa_metric(y)\n",
    "    cv = []\n",
    "    nets = []\n",
    "\n",
    "    for i, (trn_idx, val_idx) in enumerate(folds.split(X, y, group), 1):\n",
    "        U.log(f'Fold #{i}')\n",
    "        x_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n",
    "        x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        net, split_input = build_model(\n",
    "            cat_cols=cat_cols, num_cols=num_cols, \n",
    "            cat_sizes=[5], loss=root_mean_squared_error,\n",
    "            output_size=1, output_act='linear')\n",
    "\n",
    "        net.fit(x=split_input(x_trn), y=y_trn,\n",
    "                validation_data=(split_input(x_val), y_val),\n",
    "                epochs=100, batch_size=2560,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(patience=20)])\n",
    "        \n",
    "        oof[val_idx] = net.predict(split_input(x_val)).ravel()\n",
    "        cv.append(np.mean(cappa(y_val, oof[val_idx])))\n",
    "        nets.append((net, split_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0118 20:37:23.776040 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n",
      "W0118 20:37:24.054708 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n",
      "W0118 20:37:24.299551 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n",
      "W0118 20:37:24.539256 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n",
      "W0118 20:37:24.782555 140088942450496 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.frame.DataFrame'>\", \"<class 'pandas.core.series.Series'>\"}), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros((X_tst.shape[0], len(nets)), dtype=np.float32)\n",
    "for i, (net, prep) in enumerate(nets):\n",
    "    preds[:, i] = net.predict(prep(X_tst)).ravel()\n",
    "avg_preds = preds.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49116735304187487"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounder = PredictionsRoudner(y)\n",
    "y_hat, bounds = rounder(oof)\n",
    "cappa(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if U.on_kaggle():\n",
    "    nets = bundle.models(model='nets', version='001')\n",
    "    rounder = PredictionsRoudner(y)\n",
    "    y_hat, bounds = rounder(oof)\n",
    "    normalize(X_tst, num_cols, grouping_key='session_title')\n",
    "    preds = np.zeros((X_tst.shape[0], len(nets)), dtype=np.float32)\n",
    "    for i, (net, prep) in enumerate(nets):\n",
    "        preds[:, i] = net.predict(prep(X_tst)).ravel()\n",
    "    avg_preds = preds.mean(axis=1)\n",
    "    bounds = bundle.bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading relevant features list from disk.\n",
      "Total number of features: 1017\n",
      "Training model: lightgbm\n",
      "Running k-fold 1 of 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrn's rmse: 1.17672\tval's rmse: 1.18593\n",
      "[200]\ttrn's rmse: 1.11692\tval's rmse: 1.13068\n",
      "[300]\ttrn's rmse: 1.07252\tval's rmse: 1.0909\n",
      "[400]\ttrn's rmse: 1.03928\tval's rmse: 1.06257\n",
      "[500]\ttrn's rmse: 1.01117\tval's rmse: 1.03984\n",
      "[600]\ttrn's rmse: 0.989669\tval's rmse: 1.02333\n",
      "[700]\ttrn's rmse: 0.972189\tval's rmse: 1.01092\n",
      "[800]\ttrn's rmse: 0.957964\tval's rmse: 1.00189\n",
      "[900]\ttrn's rmse: 0.945652\tval's rmse: 0.994829\n",
      "[1000]\ttrn's rmse: 0.93478\tval's rmse: 0.989016\n",
      "[1100]\ttrn's rmse: 0.925694\tval's rmse: 0.984744\n",
      "[1200]\ttrn's rmse: 0.917438\tval's rmse: 0.981113\n",
      "[1300]\ttrn's rmse: 0.910309\tval's rmse: 0.978574\n",
      "[1400]\ttrn's rmse: 0.903554\tval's rmse: 0.976581\n",
      "[1500]\ttrn's rmse: 0.897332\tval's rmse: 0.97493\n",
      "[1600]\ttrn's rmse: 0.891442\tval's rmse: 0.973439\n",
      "[1700]\ttrn's rmse: 0.88591\tval's rmse: 0.972467\n",
      "[1800]\ttrn's rmse: 0.880539\tval's rmse: 0.971495\n",
      "[1900]\ttrn's rmse: 0.875372\tval's rmse: 0.970632\n",
      "[2000]\ttrn's rmse: 0.870449\tval's rmse: 0.969921\n",
      "[2100]\ttrn's rmse: 0.865753\tval's rmse: 0.969463\n",
      "[2200]\ttrn's rmse: 0.861213\tval's rmse: 0.96901\n",
      "[2300]\ttrn's rmse: 0.856783\tval's rmse: 0.968563\n",
      "[2400]\ttrn's rmse: 0.852423\tval's rmse: 0.968258\n",
      "[2500]\ttrn's rmse: 0.848211\tval's rmse: 0.967972\n",
      "[2600]\ttrn's rmse: 0.844156\tval's rmse: 0.967857\n",
      "[2700]\ttrn's rmse: 0.840153\tval's rmse: 0.967819\n",
      "Early stopping, best iteration is:\n",
      "[2672]\ttrn's rmse: 0.841246\tval's rmse: 0.967758\n",
      "Running k-fold 2 of 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrn's rmse: 1.17675\tval's rmse: 1.19454\n",
      "[200]\ttrn's rmse: 1.1168\tval's rmse: 1.1428\n",
      "[300]\ttrn's rmse: 1.07023\tval's rmse: 1.1026\n",
      "[400]\ttrn's rmse: 1.0395\tval's rmse: 1.07778\n",
      "[500]\ttrn's rmse: 1.01121\tval's rmse: 1.05439\n",
      "[600]\ttrn's rmse: 0.990244\tval's rmse: 1.03838\n",
      "[700]\ttrn's rmse: 0.972164\tval's rmse: 1.0246\n",
      "[800]\ttrn's rmse: 0.95712\tval's rmse: 1.01408\n",
      "[900]\ttrn's rmse: 0.945096\tval's rmse: 1.0067\n",
      "[1000]\ttrn's rmse: 0.934706\tval's rmse: 1.00072\n",
      "[1100]\ttrn's rmse: 0.92578\tval's rmse: 0.996443\n",
      "[1200]\ttrn's rmse: 0.917639\tval's rmse: 0.992678\n",
      "[1300]\ttrn's rmse: 0.910477\tval's rmse: 0.990337\n",
      "[1400]\ttrn's rmse: 0.903711\tval's rmse: 0.987979\n",
      "[1500]\ttrn's rmse: 0.897488\tval's rmse: 0.986351\n",
      "[1600]\ttrn's rmse: 0.891692\tval's rmse: 0.985112\n",
      "[1700]\ttrn's rmse: 0.886203\tval's rmse: 0.983888\n",
      "[1800]\ttrn's rmse: 0.880971\tval's rmse: 0.982721\n",
      "[1900]\ttrn's rmse: 0.875823\tval's rmse: 0.98186\n",
      "[2000]\ttrn's rmse: 0.871052\tval's rmse: 0.981295\n",
      "[2100]\ttrn's rmse: 0.866283\tval's rmse: 0.980378\n",
      "[2200]\ttrn's rmse: 0.861737\tval's rmse: 0.979869\n",
      "[2300]\ttrn's rmse: 0.857398\tval's rmse: 0.97961\n",
      "[2400]\ttrn's rmse: 0.853154\tval's rmse: 0.979189\n",
      "[2500]\ttrn's rmse: 0.848988\tval's rmse: 0.978925\n",
      "[2600]\ttrn's rmse: 0.844915\tval's rmse: 0.978655\n",
      "[2700]\ttrn's rmse: 0.840932\tval's rmse: 0.978351\n",
      "[2800]\ttrn's rmse: 0.837032\tval's rmse: 0.978297\n",
      "[2900]\ttrn's rmse: 0.833121\tval's rmse: 0.978035\n",
      "Early stopping, best iteration is:\n",
      "[2865]\ttrn's rmse: 0.834433\tval's rmse: 0.977968\n",
      "Running k-fold 3 of 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrn's rmse: 1.18121\tval's rmse: 1.16589\n",
      "[200]\ttrn's rmse: 1.11989\tval's rmse: 1.11399\n",
      "[300]\ttrn's rmse: 1.0747\tval's rmse: 1.07737\n",
      "[400]\ttrn's rmse: 1.04096\tval's rmse: 1.05101\n",
      "[500]\ttrn's rmse: 1.01276\tval's rmse: 1.02948\n",
      "[600]\ttrn's rmse: 0.991203\tval's rmse: 1.01442\n",
      "[700]\ttrn's rmse: 0.973556\tval's rmse: 1.00304\n",
      "[800]\ttrn's rmse: 0.959322\tval's rmse: 0.994538\n",
      "[900]\ttrn's rmse: 0.947146\tval's rmse: 0.987921\n",
      "[1000]\ttrn's rmse: 0.936248\tval's rmse: 0.98267\n",
      "[1100]\ttrn's rmse: 0.927203\tval's rmse: 0.978975\n",
      "[1200]\ttrn's rmse: 0.918896\tval's rmse: 0.976297\n",
      "[1300]\ttrn's rmse: 0.911499\tval's rmse: 0.974081\n",
      "[1400]\ttrn's rmse: 0.90478\tval's rmse: 0.972132\n",
      "[1500]\ttrn's rmse: 0.898418\tval's rmse: 0.970802\n",
      "[1600]\ttrn's rmse: 0.892527\tval's rmse: 0.969777\n",
      "[1700]\ttrn's rmse: 0.886879\tval's rmse: 0.968902\n",
      "[1800]\ttrn's rmse: 0.881578\tval's rmse: 0.968157\n",
      "[1900]\ttrn's rmse: 0.876594\tval's rmse: 0.967563\n",
      "[2000]\ttrn's rmse: 0.871638\tval's rmse: 0.967202\n",
      "[2100]\ttrn's rmse: 0.866958\tval's rmse: 0.966977\n",
      "[2200]\ttrn's rmse: 0.862444\tval's rmse: 0.966672\n",
      "[2300]\ttrn's rmse: 0.857991\tval's rmse: 0.966524\n",
      "[2400]\ttrn's rmse: 0.853725\tval's rmse: 0.966428\n",
      "[2500]\ttrn's rmse: 0.849419\tval's rmse: 0.966353\n",
      "[2600]\ttrn's rmse: 0.845404\tval's rmse: 0.966286\n",
      "[2700]\ttrn's rmse: 0.841369\tval's rmse: 0.966134\n",
      "[2800]\ttrn's rmse: 0.837389\tval's rmse: 0.965967\n",
      "Early stopping, best iteration is:\n",
      "[2797]\ttrn's rmse: 0.837517\tval's rmse: 0.965951\n",
      "Running k-fold 4 of 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrn's rmse: 1.17\tval's rmse: 1.18689\n",
      "[200]\ttrn's rmse: 1.11047\tval's rmse: 1.13527\n",
      "[300]\ttrn's rmse: 1.06626\tval's rmse: 1.09902\n",
      "[400]\ttrn's rmse: 1.02961\tval's rmse: 1.06979\n",
      "[500]\ttrn's rmse: 1.00298\tval's rmse: 1.05021\n",
      "[600]\ttrn's rmse: 0.98289\tval's rmse: 1.03684\n",
      "[700]\ttrn's rmse: 0.965048\tval's rmse: 1.02538\n",
      "[800]\ttrn's rmse: 0.951551\tval's rmse: 1.01755\n",
      "[900]\ttrn's rmse: 0.939423\tval's rmse: 1.011\n",
      "[1000]\ttrn's rmse: 0.929155\tval's rmse: 1.00618\n",
      "[1100]\ttrn's rmse: 0.919817\tval's rmse: 1.00226\n",
      "[1200]\ttrn's rmse: 0.911607\tval's rmse: 0.99934\n",
      "[1300]\ttrn's rmse: 0.904235\tval's rmse: 0.996959\n",
      "[1400]\ttrn's rmse: 0.897484\tval's rmse: 0.995075\n",
      "[1500]\ttrn's rmse: 0.891175\tval's rmse: 0.993496\n",
      "[1600]\ttrn's rmse: 0.885175\tval's rmse: 0.992161\n",
      "[1700]\ttrn's rmse: 0.879625\tval's rmse: 0.991224\n",
      "[1800]\ttrn's rmse: 0.874386\tval's rmse: 0.990487\n",
      "[1900]\ttrn's rmse: 0.869328\tval's rmse: 0.989875\n",
      "[2000]\ttrn's rmse: 0.864426\tval's rmse: 0.989423\n",
      "[2100]\ttrn's rmse: 0.859655\tval's rmse: 0.989109\n",
      "[2200]\ttrn's rmse: 0.855049\tval's rmse: 0.988729\n",
      "[2300]\ttrn's rmse: 0.850582\tval's rmse: 0.988314\n",
      "[2400]\ttrn's rmse: 0.846326\tval's rmse: 0.988029\n",
      "[2500]\ttrn's rmse: 0.842155\tval's rmse: 0.987859\n",
      "[2600]\ttrn's rmse: 0.838094\tval's rmse: 0.987569\n",
      "[2700]\ttrn's rmse: 0.834047\tval's rmse: 0.987444\n",
      "[2800]\ttrn's rmse: 0.830186\tval's rmse: 0.987397\n",
      "[2900]\ttrn's rmse: 0.826326\tval's rmse: 0.987319\n",
      "[3000]\ttrn's rmse: 0.822532\tval's rmse: 0.987589\n",
      "Early stopping, best iteration is:\n",
      "[2900]\ttrn's rmse: 0.826326\tval's rmse: 0.987319\n",
      "Running k-fold 5 of 5\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrn's rmse: 1.1776\tval's rmse: 1.19935\n",
      "[200]\ttrn's rmse: 1.11651\tval's rmse: 1.15068\n",
      "[300]\ttrn's rmse: 1.07117\tval's rmse: 1.11631\n",
      "[400]\ttrn's rmse: 1.03477\tval's rmse: 1.08969\n",
      "[500]\ttrn's rmse: 1.00559\tval's rmse: 1.06922\n",
      "[600]\ttrn's rmse: 0.983847\tval's rmse: 1.0552\n",
      "[700]\ttrn's rmse: 0.966233\tval's rmse: 1.04458\n",
      "[800]\ttrn's rmse: 0.951199\tval's rmse: 1.03643\n",
      "[900]\ttrn's rmse: 0.938075\tval's rmse: 1.02971\n",
      "[1000]\ttrn's rmse: 0.927263\tval's rmse: 1.02499\n",
      "[1100]\ttrn's rmse: 0.917721\tval's rmse: 1.02111\n",
      "[1200]\ttrn's rmse: 0.909009\tval's rmse: 1.01812\n",
      "[1300]\ttrn's rmse: 0.901278\tval's rmse: 1.01608\n",
      "[1400]\ttrn's rmse: 0.894223\tval's rmse: 1.01405\n",
      "[1500]\ttrn's rmse: 0.88775\tval's rmse: 1.01259\n",
      "[1600]\ttrn's rmse: 0.881595\tval's rmse: 1.01136\n",
      "[1700]\ttrn's rmse: 0.875845\tval's rmse: 1.01053\n",
      "[1800]\ttrn's rmse: 0.870487\tval's rmse: 1.00996\n",
      "[1900]\ttrn's rmse: 0.86519\tval's rmse: 1.00934\n",
      "[2000]\ttrn's rmse: 0.860317\tval's rmse: 1.00896\n",
      "[2100]\ttrn's rmse: 0.855671\tval's rmse: 1.00863\n",
      "[2200]\ttrn's rmse: 0.85103\tval's rmse: 1.00839\n",
      "[2300]\ttrn's rmse: 0.846557\tval's rmse: 1.0082\n",
      "[2400]\ttrn's rmse: 0.842153\tval's rmse: 1.00805\n",
      "[2500]\ttrn's rmse: 0.837964\tval's rmse: 1.00777\n",
      "[2600]\ttrn's rmse: 0.833863\tval's rmse: 1.00778\n",
      "[2700]\ttrn's rmse: 0.829871\tval's rmse: 1.00756\n",
      "[2800]\ttrn's rmse: 0.826001\tval's rmse: 1.00748\n",
      "[2900]\ttrn's rmse: 0.822159\tval's rmse: 1.00741\n",
      "[3000]\ttrn's rmse: 0.818474\tval's rmse: 1.00742\n",
      "Early stopping, best iteration is:\n",
      "[2988]\ttrn's rmse: 0.818932\tval's rmse: 1.00737\n",
      "Fold evaluation results:\n",
      "cv_cappa_1=0.6163, cv_cappa_2=0.6176, cv_cappa_3=0.5928, cv_cappa_4=0.5875, cv_cappa_5=0.5740\n",
      "Saving the trained models\n",
      "Saving the optimal rounding bounds\n",
      "Optimal bounds: [-inf, 0.5639922420507086, 1.5970792146119455, 2.1768563013529727, inf]\n"
     ]
    }
   ],
   "source": [
    "algo = 'lightgbm'\n",
    "version = '021'\n",
    "\n",
    "U.log('Loading relevant features list from disk.')\n",
    "features = [c for c in X_trn.columns \n",
    "            if c not in ('installation_id', 'game_session', 'accuracy_group')]\n",
    "U.log(f'Total number of features: {len(features)}')\n",
    "\n",
    "if U.on_kaggle():\n",
    "    U.log('Inference on Kaggle.')\n",
    "    bounds = bundle.bounds()\n",
    "    predicted = inference(X_tst, features, bounds=bounds, model=algo, version=version)\n",
    "    U.log('Saving predictions on disk.')\n",
    "    filename = submit(predicted)\n",
    "    submit_df = pd.read_csv(filename)\n",
    "    U.log('First 20 submission rows:')\n",
    "    display(submit_df.head(20))\n",
    "    \n",
    "else:\n",
    "    U.log(f'Training model: {algo}')\n",
    "    cappa = make_cappa_metric(X_trn['accuracy_group'])\n",
    "    trainer = EnsembleTrainer(algo=algo, cv_metrics={'cappa': cappa})\n",
    "    fold = GroupKFold(n_splits=5)\n",
    "    config = get_default_config(algo)\n",
    "    U.set_nested(config, 'model_params.feature_fraction', 0.5)\n",
    "    U.set_nested(config, 'model_params.bagging_fraction', 0.5)\n",
    "    U.set_nested(config, 'model_params.learning_rate', 0.003)\n",
    "    U.set_nested(config, 'model_params.bagging_freq', 1)\n",
    "    result = trainer.train(X_trn, features=features, fold=fold, config=config)\n",
    "    U.log('Saving the trained models')\n",
    "    bundle.save(result.models, f'models_{algo}_{version}')\n",
    "    U.log('Saving the optimal rounding bounds')\n",
    "    bounds = optimize_rounding_bounds(result.oof, X_trn['accuracy_group'].values)\n",
    "    U.log(f'Optimal bounds: {bounds}')\n",
    "    bundle.save(bounds, 'bounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import PredictionsRoudner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounder = PredictionsRoudner(train_target=X_trn['accuracy_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, bounds_dist = rounder(result.oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3515825283399336, 1.8037354713648037, 2.065028190612793]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff3b88b0cf8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.Series(preds)\n",
    " .value_counts()\n",
    " .rename('count')\n",
    " .reset_index()\n",
    " .sort_values(by='index')\n",
    " .plot.bar(x='index', y='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on dataset of shape: 1017\n",
      "Loading external models: lightgbm v021.\n",
      "Running models on test data...\n",
      "Averaging ensemble predictions.\n",
      "Rounding predictions using optimal bounds.\n",
      "Converting predictions into submission file.\n",
      "Running locally.\n",
      "(1000, 2) Packaging training results into dataset.\n",
      "/tmp/bowl2019/meta.joblib --> /home/ck/data/bowl2019/external/meta.joblib\n",
      "/tmp/bowl2019/bounds.joblib --> /home/ck/data/bowl2019/external/bounds.joblib\n",
      "/tmp/bowl2019/models_lightgbm_021.joblib --> /home/ck/data/bowl2019/external/models_lightgbm_021.joblib\n",
      "/tmp/bowl2019/features.joblib --> /home/ck/data/bowl2019/external/features.joblib\n",
      "/tmp/bowl2019/encoders.joblib --> /home/ck/data/bowl2019/external/encoders.joblib\n",
      "Packaging helper scripts into dataset.\n",
      "../selection.py --> /home/ck/data/bowl2019/external/selection.py\n",
      "../encode.py --> /home/ck/data/bowl2019/external/encode.py\n",
      "../features.py --> /home/ck/data/bowl2019/external/features.py\n",
      "../training.py --> /home/ck/data/bowl2019/external/training.py\n",
      "../feedback.py --> /home/ck/data/bowl2019/external/feedback.py\n",
      "../style.py --> /home/ck/data/bowl2019/external/style.py\n",
      "../basedir.py --> /home/ck/data/bowl2019/external/basedir.py\n",
      "../dataset.py --> /home/ck/data/bowl2019/external/dataset.py\n",
      "../models.py --> /home/ck/data/bowl2019/external/models.py\n",
      "../extract_features.py --> /home/ck/data/bowl2019/external/extract_features.py\n",
      "../plots.py --> /home/ck/data/bowl2019/external/plots.py\n",
      "../normalize.py --> /home/ck/data/bowl2019/external/normalize.py\n",
      "../meta.py --> /home/ck/data/bowl2019/external/meta.py\n",
      "../bundle.py --> /home/ck/data/bowl2019/external/bundle.py\n",
      "../metric.py --> /home/ck/data/bowl2019/external/metric.py\n",
      "../utils.py --> /home/ck/data/bowl2019/external/utils.py\n"
     ]
    }
   ],
   "source": [
    "if not U.on_kaggle():\n",
    "    import os\n",
    "    #features = bundle.features()\n",
    "    # bounds = bundle.bounds()\n",
    "    bounds = bounds_dist\n",
    "    filename = submit(inference(X_tst, features, bounds, model=algo, version=version))\n",
    "    assert os.path.exists(filename)\n",
    "    assert pd.read_csv(filename).shape[0] == 1000\n",
    "    bundle.package(folder='/home/ck/data/bowl2019/external/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff3b903f588>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.read_csv('submission.csv')['accuracy_group'].value_counts().reset_index().sort_values(by='index').set_index('index').plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff3b8fbaef0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_trn['accuracy_group'].value_counts().reset_index().sort_values(by='index').set_index('index').plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
